{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ebb0749",
   "metadata": {},
   "source": [
    "# Inventory management project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbb4e04",
   "metadata": {},
   "source": [
    "## Data exploration and preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37e3e88",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30932e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2836fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df_infos = pd.read_csv(\"infos.csv\", sep = \"|\")\n",
    "df_items = pd.read_csv(\"items.csv\", sep = \"|\")\n",
    "df_orders = pd.read_csv(\"orders.csv.zip\", sep = \"|\", compression=\"zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701fd006-ff0c-48e8-861a-9a1ead185b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dd59cc-88a4-42e2-8348-f67600de0cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fede9bd6-f7b0-4b06-9b1e-14fea97a4ba8",
   "metadata": {},
   "source": [
    "### Damyan: Understand the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533ef262-d0e2-45d3-9bff-6da8871f1c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa0767b-ad7d-4673-85d6-8db5435d5499",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679b8355-3f55-4ebd-acc8-e3f825b56a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_infos.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f98473f-f411-498a-8453-bc16d0a2bcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_infos.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5360dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix datetime format for transaction time\n",
    "\n",
    "df_orders['time'] = pd.to_datetime(df_orders['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40f4a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_orders[\"itemID\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3430cec9-a555-4614-828a-c5aea8156acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_order = df_orders[[\"time\", \"order\"]].copy()\n",
    "log_order[\"order\"] = np.log(log_order[\"order\"])\n",
    "fig4 = px.histogram(\n",
    "    log_order.groupby(\"time\")[\"order\"].sum(), \n",
    "    x='order', \n",
    "    title='Distribution of Log of Order Quantities per Day',\n",
    "    labels={'order': 'Log of Order Quantity', 'count': 'Frequency'},\n",
    "    nbins=30\n",
    ")\n",
    "fig4.update_layout(\n",
    "    xaxis_title='Order Quantity',\n",
    "    yaxis_title='Frequency',\n",
    "    width=800,\n",
    "    height=500\n",
    ")\n",
    "fig4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126285a8-2e05-46ba-ad82-1d754f7d6f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig5 = px.histogram(\n",
    "    df_orders, \n",
    "    x='order', \n",
    "    title='Distribution of Log of Order Quantities per Day',\n",
    "    labels={'order': 'Log of Order Quantity', 'count': 'Frequency'},\n",
    "    nbins=30\n",
    ")\n",
    "fig5.update_layout(\n",
    "    xaxis_title='Order Quantity',\n",
    "    yaxis_title='Frequency',\n",
    "    width=800,\n",
    "    height=500\n",
    ")\n",
    "fig5.show()\n",
    "\n",
    "log_order = df_orders[[\"date\", \"order\"]].copy()\n",
    "log_order[\"order\"] = np.log(log_order[\"order\"])\n",
    "fig6 = px.histogram(\n",
    "    log_order, \n",
    "    x='order',\n",
    "    title='Distribution of Log of Order Quantities per Day',\n",
    "    labels={'order': 'Log of Order Quantity', 'count': 'Frequency'},\n",
    "    nbins=30\n",
    ")\n",
    "fig6.update_layout(\n",
    "    xaxis_title='Order Quantity',\n",
    "    yaxis_title='Frequency',\n",
    "    width=800,\n",
    "    height=500\n",
    ")\n",
    "fig6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981929c5-c2b0-4e86-b2fa-48987e54c7bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig7 = px.histogram(\n",
    "    df_items, \n",
    "    x='customerRating', \n",
    "    title='Distribution of Customer Rating',\n",
    "    labels={'order': 'Customer Rating', 'count': 'Frequency'},\n",
    "    nbins=30\n",
    ")\n",
    "fig7.update_layout(\n",
    "    xaxis_title='Customer Rating',\n",
    "    yaxis_title='Frequency',\n",
    "    width=800,\n",
    "    height=500\n",
    ")\n",
    "fig7.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be9ffd8-4d87-4dbc-9229-bc6a0645133d",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_price = df_orders[[\"order\", \"salesPrice\"]].copy()\n",
    "item_price[\"item_price\"] = item_price[\"salesPrice\"] / item_price[\"order\"]\n",
    "fig8 = px.histogram(\n",
    "    item_price, \n",
    "    x='item_price', \n",
    "    title='Distribution of Product Price per Sale',\n",
    "    labels={'order': 'Prices', 'count': 'Frequency'},\n",
    "    nbins=30\n",
    ")\n",
    "fig8.update_layout(\n",
    "    xaxis_title='Prices',\n",
    "    yaxis_title='Count',\n",
    "    width=800,\n",
    "    height=500\n",
    ")\n",
    "fig8.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96055a2-7dee-4717-a174-80bf7829ef33",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_price = df_orders[[\"order\", \"salesPrice\"]].copy()\n",
    "item_price[\"item_price\"] = item_price[\"salesPrice\"] / item_price[\"order\"]\n",
    "log_sales = item_price[\"item_price\"].copy()\n",
    "log_sales = np.log(log_sales)\n",
    "fig9 = px.histogram(\n",
    "    log_sales, \n",
    "    title='Distribution of Log of Product Prices per Sale',\n",
    "    labels={'order': 'Log Prices', 'count': 'Frequency'},\n",
    "    nbins=30\n",
    ")\n",
    "fig9.update_layout(\n",
    "    xaxis_title='Log Prices',\n",
    "    yaxis_title='Count',\n",
    "    width=800,\n",
    "    height=500\n",
    ")\n",
    "fig9.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e12ee2c-41a9-4fe6-bce0-b1716663918a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig10 = px.line(\n",
    "    df_orders.groupby(by=\"time\", as_index=False)[\"order\"].sum(),\n",
    "    x=\"time\",\n",
    "    y=\"order\",\n",
    "    title='Number of Sales per Day'\n",
    ")\n",
    "fig10.update_layout(\n",
    "    xaxis_title='Time',\n",
    "    yaxis_title='Sales',\n",
    "    width=800,\n",
    "    height=500\n",
    ")\n",
    "fig10.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0396da9-846f-4e9f-ada9-e36b86793c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue = df_orders.copy()\n",
    "revenue[\"time\"] = pd.to_datetime(revenue[\"time\"]).dt.date\n",
    "revenue[\"revenue\"] = revenue[\"order\"] * revenue[\"salesPrice\"]\n",
    "fig11 = px.line(\n",
    "    revenue.groupby(by=\"time\", as_index=False)[\"revenue\"].sum(),\n",
    "    x=\"time\",\n",
    "    y=\"revenue\",\n",
    "    title='Revenue per Day'\n",
    ")\n",
    "fig11.update_layout(\n",
    "    xaxis_title='Time',\n",
    "    yaxis_title='Revenue',\n",
    "    width=800,\n",
    "    height=500\n",
    ")\n",
    "fig11.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4164bcf6-2033-4530-a088-0a6c812bef19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "# Sales data (from result)\n",
    "sales_data = df_orders.groupby(by=\"date\", as_index=False)[\"order\"].sum()\n",
    "\n",
    "# Revenue data (from orders)\n",
    "revenue = df_orders.copy()\n",
    "revenue[\"date\"] = pd.to_datetime(revenue[\"time\"]).dt.date\n",
    "revenue[\"revenue\"] = revenue[\"order\"] * revenue[\"salesPrice\"]\n",
    "revenue_data = revenue.groupby(by=\"date\", as_index=False)[\"revenue\"].sum()\n",
    "\n",
    "# Create subplots with secondary y-axis\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "# Add sales trace (primary y-axis)\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=sales_data[\"date\"],\n",
    "        y=sales_data[\"order\"],\n",
    "        mode='lines',\n",
    "        name='Total Orders',\n",
    "        line=dict(color='blue', width=2),\n",
    "        hovertemplate='<b>Date:</b> %{x}<br><b>Orders:</b> %{y}<extra></extra>'\n",
    "    ),\n",
    "    secondary_y=False,\n",
    ")\n",
    "\n",
    "# Add revenue trace (secondary y-axis)\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=revenue_data[\"date\"],\n",
    "        y=revenue_data[\"revenue\"],\n",
    "        mode='lines',\n",
    "        name='Revenue',\n",
    "        line=dict(color='red', width=2),\n",
    "        hovertemplate='<b>Date:</b> %{x}<br><b>Revenue:</b> $%{y:,.2f}<extra></extra>'\n",
    "    ),\n",
    "    secondary_y=True,\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Sales Orders and Revenue Over Time',\n",
    "    xaxis_title='Date',\n",
    "    width=1000,\n",
    "    height=600,\n",
    "    hovermode='x unified',\n",
    "    legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=0.99,\n",
    "        xanchor=\"left\",\n",
    "        x=0.01\n",
    "    )\n",
    ")\n",
    "\n",
    "# Set y-axes titles\n",
    "fig.update_yaxes(title_text=\"Number of Orders\", secondary_y=False, color='blue')\n",
    "fig.update_yaxes(title_text=\"Revenue ($)\", secondary_y=True, color='red')\n",
    "\n",
    "# Optional: Style the y-axis text colors to match the lines\n",
    "fig.update_yaxes(tickfont=dict(color='blue'), secondary_y=False)\n",
    "fig.update_yaxes(tickfont=dict(color='red'), secondary_y=True)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc1d78d-37f6-4ac0-b2fc-a96b3c7a1846",
   "metadata": {},
   "source": [
    "### Damyan: df_items preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb503303-6a44-40b0-98e3-720c5184ff51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column idicating if customer rating is missing\n",
    "items[\"customerRatingIndicator\"] = items[\"customerRating\"] == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a40bab0-f3fd-40c8-ae53-a3241bedaccf",
   "metadata": {},
   "source": [
    "### df_items preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef63620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there seem to be 0 price transactions\n",
    "\n",
    "len(df_orders[df_orders[\"salesPrice\"]==0])/len(df_orders)*100\n",
    "\n",
    "# they are 0.02% of all transactions, it is best to delete them instead of thinking how to handle them\n",
    "\n",
    "df_orders = df_orders[df_orders[\"salesPrice\"]!=0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0244298f",
   "metadata": {},
   "source": [
    "### Train and Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e536694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the split date\n",
    "split_date = pd.to_datetime(\"08.06.2018\", format=\"%d.%m.%Y\")\n",
    "\n",
    "# Split into train and test\n",
    "df_test = df_orders[df_orders['time'] > split_date]\n",
    "df_train = df_orders[df_orders['time'] <= split_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c019c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we saw that we lose around 1000 items with the train-test split, because they have orders only in the last 3 weeks\n",
    "# so we check their price x quantity (revenue) - what % it is of the total revenue\n",
    "# to see if we lose a lot with this cropping and decide how to continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5080ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"revenue\"] = df_test[\"order\"] * df_test[\"salesPrice\"]\n",
    "df_orders[\"revenue\"] = df_orders[\"order\"] * df_orders[\"salesPrice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99c3fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_items = pd.DataFrame(df_test[~df_test[\"itemID\"].isin(df_train[\"itemID\"])][\"itemID\"].unique(), columns = [\"itemID\"])\n",
    "missing_items[\"will_be_lost\"] = \"yes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196881c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders = df_orders.merge(missing_items, how = \"left\", on = \"itemID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f7d2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders[\"revenue\"] = df_orders[\"salesPrice\"] * df_orders[\"order\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667cd0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "round((df_orders[df_orders[\"will_be_lost\"] == \"yes\"][\"revenue\"].sum()/df_orders[\"revenue\"].sum())*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54581d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# they account for 5.09% of the total revenue of the historical 6-month data we have\n",
    "# it is low enough, we continue like this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5524485d",
   "metadata": {},
   "source": [
    "### df_train preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebff7530",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b7d06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add one column which is only with the date, no time\n",
    "\n",
    "df_train['date'] = df_train['time'].dt.date\n",
    "df_train['date'] = pd.to_datetime(df_train['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abf6da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do all items have an order in the period we have?\n",
    "\n",
    "round(df_train[\"itemID\"].nunique()/len(df_items)*100, 2)\n",
    "\n",
    "# 94.05% of all items have an order in the period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149e679c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# which transactions are performed on a discounted price?\n",
    "# we assume the following: a transaction is marked as having a promotion if, for the same item, somewhere else in the table there is\n",
    "# another transaction performed on a lower price\n",
    "\n",
    "# Step 1: Get the maximum price per itemID\n",
    "max_price_per_item = df_train.groupby('itemID')['salesPrice'].transform('max')\n",
    "\n",
    "# Step 2: Compare each row's price to the max price for that item\n",
    "df_train['promotion'] = df_train['salesPrice'] < max_price_per_item\n",
    "\n",
    "# Step 3: Convert boolean to \"yes\"/\"no\"\n",
    "df_train['promotion'] = df_train['promotion'].map({True: 1, False: 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef5ac76-397d-440b-a9a0-1d096be0f423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total order quantity per promotion category\n",
    "df_train.groupby(\"promotion\")[\"order\"].sum() / df_train.groupby(\"promotion\")[\"order\"].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e825c88-6763-4bea-80ec-ae24fd24fe45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revenue per promotion category per unit of transaction number\n",
    "df_train.groupby(\"promotion\")[[\"order\", \"salesPrice\"]].apply(lambda x: (x[\"order\"]*x[\"salesPrice\"]).sum()) / df_train.groupby(\"promotion\")[\"order\"].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09750247-90d7-4863-b79f-4cb2310b7f2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Option 1: Simple histogram of order per day\n",
    "fig1 = px.histogram(\n",
    "    df_train.groupby(\"time\")[\"order\"].sum(), \n",
    "    x='order', \n",
    "    title='Distribution of Order Quantities per Day',\n",
    "    labels={'order': 'Order Quantity', 'count': 'Count'},\n",
    "    nbins=30\n",
    ")\n",
    "fig1.update_layout(\n",
    "    xaxis_title='Order Quantity',\n",
    "    yaxis_title='Count',\n",
    "    width=800,\n",
    "    height=500\n",
    ")\n",
    "fig1.show()\n",
    "\n",
    "order_totals = df_orders.groupby(\"itemID\")[\"order\"].sum()\n",
    "item_stats = df_orders.groupby(\"itemID\").agg({\n",
    "    \"order\": \"sum\",\n",
    "    \"in_promotion\": \"any\"  # or \"any\" if you want items that were ever in promotion\n",
    "}).reset_index()\n",
    "promo_orders = item_stats[item_stats['in_promotion'] == True]['order']\n",
    "regular_orders = item_stats[item_stats['in_promotion'] == False]['order']\n",
    "\n",
    "# Option 2: Simple histogram of order per day\n",
    "fig2 = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=[\n",
    "        'Distribution of Order Quantities per Item',\n",
    "        'Distribution of Order Quantities by Promotion Status'\n",
    "    ],\n",
    "    horizontal_spacing=0.1\n",
    ")\n",
    "\n",
    "# First subplot (same as above)\n",
    "fig2.add_trace(\n",
    "    go.Histogram(\n",
    "        x=order_totals,\n",
    "        name=\"Total Orders per Item\",\n",
    "        showlegend=False,\n",
    "        opacity=0.7,\n",
    "        nbinsx=30\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Second subplot with side-by-side bars\n",
    "fig2.add_trace(\n",
    "    go.Histogram(\n",
    "        x=regular_orders,\n",
    "        name=\"Regular Price\",\n",
    "        opacity=0.7,\n",
    "        nbinsx=30,\n",
    "        marker_color='blue'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig2.add_trace(\n",
    "    go.Histogram(\n",
    "        x=promo_orders,\n",
    "        name=\"In Promotion\",\n",
    "        opacity=0.7,\n",
    "        nbinsx=30,\n",
    "        marker_color='red'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "fig2.update_layout(\n",
    "    title_text=\"Distribution of Order Quantities (Side-by-side)\",\n",
    "    height=600,\n",
    "    width=1200,\n",
    "    barmode='group'  # Side-by-side bars\n",
    ")\n",
    "\n",
    "fig2.update_xaxes(title_text=\"Total Order Quantity\", row=1, col=1)\n",
    "fig2.update_xaxes(title_text=\"Total Order Quantity\", row=1, col=2)\n",
    "fig2.update_yaxes(title_text=\"Number of Items\", row=1, col=1)\n",
    "fig2.update_yaxes(title_text=\"Number of Items\", row=1, col=2)\n",
    "\n",
    "fig2.show()\n",
    "\n",
    "# 3 Price distribution\n",
    "fig3 = px.histogram(\n",
    "    df_orders,\n",
    "    x='weightedAveragePrice', # TODO: Change it with the column for order quantity weighted price\n",
    "    title='Distribution of Weighted Average Price',\n",
    "    labels={'weightedAveragePrice': 'Weighted Average Price', 'count': 'Frequency'},\n",
    "    nbins=30\n",
    ")\n",
    "fig3.update_layout(\n",
    "    xaxis_title='Weighted Average Price',\n",
    "    yaxis_title='Frequency',\n",
    "    width=800,\n",
    "    height=500\n",
    ")\n",
    "fig3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1874c69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this data frame will be aggregared on day level for final use\n",
    "# so it is best if we continue the transformation in the aggregated version\n",
    "# but before aggregation, we should check what % of items have been sold on a different price in the same day\n",
    "\n",
    "price_variations = (\n",
    "    df_train\n",
    "    .assign(date=df_train['time'].dt.date)\n",
    "    .groupby(['itemID', 'date'])['salesPrice']\n",
    "    .nunique()\n",
    "    .reset_index(name='unique_price_count')\n",
    ")\n",
    "\n",
    "\n",
    "# Filter where price count > 1 (i.e., same item sold at multiple prices)\n",
    "price_variations[price_variations['unique_price_count'] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a58915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# % of such cases from all items\n",
    "\n",
    "((price_variations[price_variations['unique_price_count'] > 1]['unique_price_count'].count())/len(df_items))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c868af23",
   "metadata": {},
   "source": [
    "### df_infos preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c28ce1e",
   "metadata": {},
   "source": [
    "#### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972e7526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in df_infos column promotion there are cells with more than one date, separated by a comma\n",
    "# how many such are there?\n",
    "\n",
    "(df_infos[\"promotion\"].str.len() > 10).sum()\n",
    "\n",
    "# 190\n",
    "# I leave it as text for now, we should handle it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea27e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_infos[\"promotion\"][df_infos[\"promotion\"].str.len() > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95580eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# does df_infos, containing the promotions, contain unique item IDs or are they duplicated?\n",
    "# I expect them to be unique\n",
    "\n",
    "len(df_infos[\"itemID\"]) == len(df_items)\n",
    "df_infos[\"itemID\"].value_counts().max() == 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23756374",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_infos[\"itemID\"].isin(df_items[\"itemID\"]).count() == len(df_infos[\"itemID\"])\n",
    "\n",
    "# it contains a row for each itemID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d81322",
   "metadata": {},
   "source": [
    "#### Deriving discounts for the simulation period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e001db37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"maxPrice\"] = df_train.groupby(\"itemID\")[\"salesPrice\"].transform(\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d802ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how do discounts vary\n",
    "\n",
    "df_train[\"discountAmount\"] = round(df_train[\"maxPrice\"] - df_train[\"salesPrice\"], 2)\n",
    "\n",
    "df_train[\"discountPerc\"] = round(df_train[\"discountAmount\"]/df_train[\"maxPrice\"], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88c39d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max and min % discount\n",
    "\n",
    "print(max(df_train[\"discountPerc\"]), min(df_train[\"discountPerc\"][df_train[\"discountPerc\"] != 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757d1c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the histogram and get the bars\n",
    "ax = df_train[\"discountPerc\"].plot(kind=\"hist\", bins=10, edgecolor='black')\n",
    "\n",
    "# Add value labels on top of each bar\n",
    "for patch in ax.patches:\n",
    "    height = patch.get_height()\n",
    "    if height > 0:\n",
    "        ax.annotate(f'{int(height)}', \n",
    "                    xy=(patch.get_x() + patch.get_width() / 2, height), \n",
    "                    xytext=(0, 5),  # offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "plt.xlabel(\"Discount Percentage\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of Discount Percentage\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e788681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of observations\n",
    "total = len(df_train[\"discountPerc\"].dropna())\n",
    "\n",
    "# Plot the histogram as density (normalized)\n",
    "ax = df_train[\"discountPerc\"].plot(kind=\"hist\", bins=10, edgecolor='black', density=False)\n",
    "\n",
    "# Get the actual bin heights (counts) to calculate percentages\n",
    "counts, bins, patches = plt.hist(df_train[\"discountPerc\"].dropna(), bins=10, edgecolor='black')\n",
    "\n",
    "# Annotate bars with percentage labels\n",
    "for count, patch in zip(counts, patches):\n",
    "    percentage = 100 * count / total\n",
    "    if count > 0:\n",
    "        plt.annotate(f'{percentage:.1f}%', \n",
    "                     xy=(patch.get_x() + patch.get_width() / 2, count), \n",
    "                     xytext=(0, 5),\n",
    "                     textcoords=\"offset points\",\n",
    "                     ha='center', va='bottom')\n",
    "\n",
    "plt.xlabel(\"Discount Percentage\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Histogram of Discount Percentage (with % labels)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26eab71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the skewed distribution, for getting an approximate discount percentage per item\n",
    "# it would be better to use the median instead of the mean\n",
    "# adding column for discounted price to table df_infos = simulation price - median discount for item\n",
    "# Start with itemID column\n",
    "df_discount_stats = df_items[[\"itemID\"]].copy()\n",
    "\n",
    "# Filter out rows where discountAmount is 0\n",
    "df_nonzero_discounts = df_train[df_train[\"discountAmount\"] != 0].copy()\n",
    "\n",
    "# Drop duplicates to keep only unique discount percentages per item\n",
    "unique_discounts = df_nonzero_discounts.drop_duplicates(subset=[\"itemID\", \"discountPerc\"])\n",
    "\n",
    "# Now compute the median of these unique values per item\n",
    "median_discounts = (\n",
    "    unique_discounts\n",
    "    .groupby(\"itemID\")[\"discountPerc\"]\n",
    "    .median()\n",
    "    .round(2)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"discountPerc\": \"medianDiscPerc\"})\n",
    ")\n",
    "\n",
    "# Merge into df_discount_stats\n",
    "df_discount_stats = df_discount_stats.merge(median_discounts, on=\"itemID\", how=\"left\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501c1962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding column for discounted price to table df_infos = simulation price - median discount for item\n",
    "\n",
    "df_infos = df_infos.merge(df_discount_stats[['itemID', 'medianDiscPerc']], on='itemID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1963db7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding column for discounted price to table df_infos = simulation price - median discount for item\n",
    "\n",
    "df_infos[\"discountedPrice\"] = np.where(\n",
    "    df_infos[\"promotion\"].notna(),\n",
    "    round(df_infos[\"simulationPrice\"] * (1 - df_infos[\"medianDiscPerc\"]), 2),\n",
    "    np.nan  # or just leave it to default if you prefer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afc1dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfinished - we have to use some mean based on similar items to derive median discount % for items which will have\n",
    "# a promotion in the simulation period but have not had a discount in the historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3ea76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding also min price per item in the orders data frame for completion\n",
    "\n",
    "df_train[\"minPrice\"] = df_train.groupby(\"itemID\")[\"salesPrice\"].transform(\"min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4570e5cb",
   "metadata": {},
   "source": [
    "#### Quickly check relation - qty sold and promotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2785884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# promo tests\n",
    "\n",
    "# Step 1: Sum quantity per itemID, date, and promotion (daily sales)\n",
    "daily_qty = (\n",
    "    df_train\n",
    "    .groupby(['itemID', 'date', 'promotion'])['order']\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Step 2: Aggregate by itemID and promotion: \n",
    "# total quantity sold (sum of daily sums)\n",
    "# count of days with sales (number of unique days)\n",
    "agg = daily_qty.groupby(['itemID', 'promotion']).agg(\n",
    "    total_qty=('order', 'sum'),\n",
    "    count_days=('date', 'nunique')\n",
    ").unstack(fill_value=0)\n",
    "\n",
    "# Step 3: Build the final DataFrame safely extracting promo/no promo columns\n",
    "summary = pd.DataFrame({\n",
    "    'QTY_no_promo': agg['total_qty'].get(0, pd.Series(0)),\n",
    "    'QTY_promo': agg['total_qty'].get(1, pd.Series(0)),\n",
    "    'count_days_no_promo': agg['count_days'].get(0, pd.Series(0)),\n",
    "    'count_days_promo': agg['count_days'].get(1, pd.Series(0))\n",
    "}).reset_index()\n",
    "\n",
    "# Step 4: Calculate average quantity per day (handle division by zero)\n",
    "summary['QTY_no_promo_per_day'] = summary.apply(\n",
    "    lambda r: r['QTY_no_promo'] / r['count_days_no_promo'] if r['count_days_no_promo'] > 0 else 0,\n",
    "    axis=1\n",
    ")\n",
    "summary['QTY_promo_per_day'] = summary.apply(\n",
    "    lambda r: r['QTY_promo'] / r['count_days_promo'] if r['count_days_promo'] > 0 else 0,\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d1a439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# promo tests\n",
    "\n",
    "len(summary[summary[\"QTY_promo_per_day\"] > summary[\"QTY_no_promo_per_day\"]])/len(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abac634a",
   "metadata": {},
   "source": [
    "## Aggregate orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b73b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate df_train on a daily basis\n",
    "# sum of QTY\n",
    "# average of price (or median?)?\n",
    "# promotion - if 1 is present, then 1 (had at least 1 promotion in that day)\n",
    "# median discount %?\n",
    "# median discount amount?\n",
    "\n",
    "# to make a desicion wether to use mean of median for price, discount amount, discount perc\n",
    "# we have to look at the distribution of the prices for some items\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e273a6",
   "metadata": {},
   "source": [
    "### Checking price per item distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a469294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a column with item_prices_count to df_train\n",
    "\n",
    "df_train[\"item_prices_count\"] = df_train.groupby(\"itemID\")[\"salesPrice\"].transform(lambda x: x.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d46af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting a random sample where the item has price discount of > 0.79, meaning there might\n",
    "# be great price variations of the item\n",
    "\n",
    "df_sample = df_train[[\"itemID\"]][df_train[\"discountPerc\"] > 0.79]\n",
    "\n",
    "df_sample = df_sample.sample(n=50, random_state=222)\n",
    "\n",
    "df_sample = df_sample.sort_values(by=\"itemID\", ascending=True)\n",
    "\n",
    "df_sample = df_sample.merge(df_train, how=\"left\", on=\"itemID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f9cc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing \n",
    "\n",
    "# Unique items\n",
    "item_ids = df_sample['itemID'].unique()\n",
    "\n",
    "# Set up the grid\n",
    "rows, cols = 10, 5\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(20, 15), sharex=False, sharey=False)\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot histogram for each item\n",
    "for i, item_id in enumerate(item_ids):\n",
    "    ax = axes[i]\n",
    "    item_prices = df_sample[df_sample['itemID'] == item_id]['salesPrice']\n",
    "\n",
    "    ax.hist(item_prices, bins=10, color='skyblue', edgecolor='black')\n",
    "    ax.set_title(f'Item {item_id}', fontsize=8)\n",
    "    ax.tick_params(labelsize=6)\n",
    "    ax.grid(True)\n",
    "\n",
    "# Hide unused subplots\n",
    "for j in range(len(item_ids), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df63ca64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it would be useful to see another check to make aa decision\n",
    "# see the top 10 items for which the mean and median are the most different and\n",
    "# see which one makes more sence for us\n",
    "\n",
    "# Group by itemID and compute mean and median\n",
    "price_stats = df_train.groupby('itemID')['salesPrice'].agg(\n",
    "    mean_price='mean',\n",
    "    median_price='median'\n",
    ").reset_index()\n",
    "\n",
    "# Compute absolute difference\n",
    "price_stats['abs_diff'] = (price_stats['mean_price'] - price_stats['median_price']).abs()\n",
    "\n",
    "# Compute absolute percentage difference relative to median\n",
    "price_stats['abs_perc_diff'] = (price_stats['abs_diff'] / price_stats['median_price']).abs() * 100\n",
    "\n",
    "# Round numerical columns\n",
    "price_stats[['mean_price', 'median_price', 'abs_diff', 'abs_perc_diff']] = price_stats[\n",
    "    ['mean_price', 'median_price', 'abs_diff', 'abs_perc_diff']].round(2)\n",
    "\n",
    "# Sort by absolute percentage difference descending\n",
    "price_stats = price_stats.sort_values(by='abs_perc_diff', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256597e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now look at histograms of top 50 of items with most % difference of mean and median\n",
    "\n",
    "df_sample = price_stats.sort_values(by='abs_perc_diff', ascending=False).head(50)[[\"itemID\"]]\n",
    "\n",
    "df_sample = df_sample.sample(n=50, random_state=222)\n",
    "\n",
    "df_sample = df_sample.sort_values(by=\"itemID\", ascending=True)\n",
    "\n",
    "df_sample = df_sample.merge(df_train, how=\"left\", on=\"itemID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcfd246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing \n",
    "\n",
    "# Unique items\n",
    "item_ids = df_sample['itemID'].unique()\n",
    "\n",
    "# Set up the grid\n",
    "rows, cols = 10, 5\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(20, 15), sharex=False, sharey=False)\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot histogram for each item\n",
    "for i, item_id in enumerate(item_ids):\n",
    "    ax = axes[i]\n",
    "    item_prices = df_sample[df_sample['itemID'] == item_id]['salesPrice']\n",
    "\n",
    "    ax.hist(item_prices, bins=10, color='skyblue', edgecolor='black')\n",
    "    ax.set_title(f'Item {item_id}', fontsize=8)\n",
    "    ax.tick_params(labelsize=6)\n",
    "    ax.grid(True)\n",
    "\n",
    "# Hide unused subplots\n",
    "for j in range(len(item_ids), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e719f780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is better to use mean because the extreme values are not a one-case accidental thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1415bd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate using mean (average), but specifically weighted average, to gain price per each\n",
    "# weighted average will take into account how accidental the outlier prices were"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e49f5b",
   "metadata": {},
   "source": [
    "### Aggregate orders on day level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1227cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to get weighted average price per each for the items\n",
    "# first we need to add a column to df_train\n",
    "# with order value = qty * price\n",
    "\n",
    "df_train[\"orderValue\"] = df_train[\"order\"] * df_train[\"salesPrice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa9740d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregating on day level\n",
    "\n",
    "df_orders_daily = df_train.groupby(['date', 'itemID']).agg(\n",
    "    qty_sold=('order', 'sum'),\n",
    "    sales_value=('orderValue', 'sum'),\n",
    "    promotion=('promotion', 'max'),  # If any transaction had promotion == 1, result will be 1\n",
    "    maxItemPrice=('maxPrice', 'max'), # doesn't matter min or max - its the same value for all transactions with the same item\n",
    "    minItemPrice=('minPrice', 'max') # doesn't matter min or max - its the same value for all transactions with the same item\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc99583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deriving price per each for the items\n",
    "# checkp change to last mode\n",
    "\n",
    "df_orders_daily[\"TotalItemQty\"] = df_orders_daily.groupby(\"itemID\")[\"qty_sold\"].transform(\"sum\")\n",
    "df_orders_daily[\"TotalItemValue\"] = df_orders_daily.groupby(\"itemID\")[\"sales_value\"].transform(\"sum\")\n",
    "\n",
    "df_orders_daily[\"PricePerEach\"] = round(df_orders_daily[\"TotalItemValue\"] / df_orders_daily[\"TotalItemQty\"], 2)\n",
    "\n",
    "df_orders_daily = df_orders_daily.drop(['TotalItemQty', 'TotalItemValue'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8ad04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deriving price per each for the items, but specific price for that day\n",
    "\n",
    "df_orders_daily[\"DailyItemQty\"] = df_orders_daily.groupby([\"itemID\", \"date\"])[\"qty_sold\"].transform(\"sum\")\n",
    "df_orders_daily[\"DailyItemValue\"] = df_orders_daily.groupby([\"itemID\", \"date\"])[\"sales_value\"].transform(\"sum\")\n",
    "\n",
    "df_orders_daily[\"PricePerEachToday\"] = round(df_orders_daily[\"DailyItemValue\"] / df_orders_daily[\"DailyItemQty\"], 2)\n",
    "\n",
    "df_orders_daily = df_orders_daily.drop(['DailyItemQty', 'DailyItemValue'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63381aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add median discout for the items\n",
    "# we have it currently for each item in df_infos\n",
    "\n",
    "# Select only itemID and medianDiscount from df_infos and merge on itemID\n",
    "df_orders_daily = df_orders_daily.merge(\n",
    "    df_infos[[\"itemID\", \"medianDiscPerc\"]],\n",
    "    how=\"left\",\n",
    "    on=\"itemID\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63d904b",
   "metadata": {},
   "source": [
    "## Add features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c3310a",
   "metadata": {},
   "source": [
    "#### Complete main DF with missing date + item combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3279e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# completing our main data frame with all missing day+item combinations\n",
    "# for them qty_sold = 0, sales_value = 0, promotion = 0, maxItemPrice = maxItemPrice, minItemPrice = minItemPrice\n",
    "# PricePerEach = mean PricePerEach for that item, medianDiscPerc = medianDiscPerc\n",
    "\n",
    "new_df = pd.DataFrame(df_orders_daily[\"itemID\"].unique(), columns = [\"itemID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639216b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create date range\n",
    "date_range = pd.date_range(start='2018-01-01', end='2018-06-07', freq='D')\n",
    "\n",
    "# Create DataFrame\n",
    "df_dates = pd.DataFrame({'date': date_range})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3701829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a dummy key to both DataFrames\n",
    "df_dates[\"key\"] = 1\n",
    "new_df[\"key\"] = 1\n",
    "\n",
    "# Perform cross join\n",
    "new_df = pd.merge(df_dates, new_df, on=\"key\").drop(\"key\", axis=1)\n",
    "\n",
    "# Sort by date and then itemID\n",
    "new_df = new_df.sort_values(by=[\"date\", \"itemID\"]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c23aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_daily = new_df.merge(df_orders_daily, how=\"left\", on=[\"date\", \"itemID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb56a758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for them qty_sold = 0, sales_value = 0, promotion = 0\n",
    "\n",
    "df_orders_daily[[\"qty_sold\", \"sales_value\", \"promotion\"]] = df_orders_daily[[\"qty_sold\", \"sales_value\", \"promotion\"]].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971cb53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maxItemPrice = maxItemPrice, minItemPrice = minItemPrice, PricePerEach = mean PricePerEach for that item, medianDiscPerc = medianDiscPerc\n",
    "\n",
    "# Fill missing maxItemPrice with the max per itemID\n",
    "df_orders_daily[\"maxItemPrice\"] = df_orders_daily.groupby(\"itemID\")[\"maxItemPrice\"].transform(lambda x: x.fillna(x.max()))\n",
    "\n",
    "# Fill missing minItemPrice with the max per itemID\n",
    "df_orders_daily[\"minItemPrice\"] = df_orders_daily.groupby(\"itemID\")[\"minItemPrice\"].transform(lambda x: x.fillna(x.max()))\n",
    "\n",
    "# Fill missing PricePerEach with the max per itemID\n",
    "df_orders_daily[\"PricePerEach\"] = df_orders_daily.groupby(\"itemID\")[\"PricePerEach\"].transform(lambda x: x.fillna(x.max()))\n",
    "\n",
    "# Fill missing PricePerEachToday with the mean per itemID\n",
    "df_orders_daily[\"PricePerEachToday\"] = df_orders_daily.groupby(\"itemID\")[\"PricePerEachToday\"].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "df_orders_daily[\"PricePerEachToday\"] = df_orders_daily[\"PricePerEachToday\"].round(2)\n",
    "\n",
    "# Fill missing medianDiscPerc with the max per itemID\n",
    "df_orders_daily[\"medianDiscPerc\"] = df_orders_daily.groupby(\"itemID\")[\"medianDiscPerc\"].transform(lambda x: x.fillna(x.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d895f1b",
   "metadata": {},
   "source": [
    "#### Add masterdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b90805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# include masterdata\n",
    "\n",
    "df_orders_daily = df_orders_daily.merge(df_items, how=\"left\", on=\"itemID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d660dc7a",
   "metadata": {},
   "source": [
    "#### Add date features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca058ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add date features\n",
    "df_orders_daily[\"weekDay\"] = df_orders_daily[\"date\"].dt.weekday + 1\n",
    "df_orders_daily[\"day\"] = df_orders_daily[\"date\"].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b54baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_week_of_month(date):\n",
    "    # First day of the month\n",
    "    first_day = date.replace(day=1)\n",
    "    \n",
    "    # Find the day of the week the first day lands on (Monday=0, Sunday=6)\n",
    "    first_day_weekday = first_day.weekday()\n",
    "    \n",
    "    # Calendar row index = (day of month + offset from Monday) // 7 + 1\n",
    "    return ((date.day + first_day_weekday - 1) // 7) + 1\n",
    "\n",
    "# Apply the function to create the column\n",
    "df_orders_daily['weekOfMonth'] = df_orders_daily['date'].apply(get_week_of_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37d5416",
   "metadata": {},
   "source": [
    "### FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4445ffd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_harmonics(data, num_harmonics=10, return_wave=None):\n",
    "    all_coefs = np.fft.fft(data)\n",
    "    coeffs = []\n",
    "    nh = return_wave + 1 if return_wave is not None else num_harmonics\n",
    "    for i in range(1, nh + 1):\n",
    "        coeffs.append(np.zeros(len(all_coefs), dtype=complex))\n",
    "        coeffs[-1][i] = all_coefs[i]\n",
    "        coeffs[-1][-i] = all_coefs[-i]\n",
    "\n",
    "    if return_wave is not None:\n",
    "        rc = np.zeros(len(all_coefs), dtype=complex) + coeffs[return_wave]\n",
    "        rc = np.fft.ifft(rc).real\n",
    "        return rc\n",
    "\n",
    "    reconstructed_coeffs = np.zeros(len(all_coefs), dtype=complex)\n",
    "    for i in range(num_harmonics):\n",
    "        reconstructed_coeffs += coeffs[i]\n",
    "    reconstructed_signal = np.fft.ifft(reconstructed_coeffs).real\n",
    "    reconstructed_signal += data.mean()\n",
    "    return reconstructed_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d40d71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    df_orders_daily[f'harmonic_{i}'] = df_orders_daily.groupby(by=\"itemID\")[\"qty_sold\"].transform(lambda c: get_harmonics(c, return_wave=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f01728c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_daily.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a20a30",
   "metadata": {},
   "source": [
    "### Cumulative variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ec1d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_daily['cum_sum_order'] = df_orders_daily.groupby('itemID')['qty_sold'].cumsum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f801a5f4",
   "metadata": {},
   "source": [
    "### Rolling statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e88f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# once again, making sure data frame is sorted by date and item ID so the rolling stats are OK\n",
    "\n",
    "df_orders_daily.sort_values(['itemID', 'date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffdedab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_daily['rolling_qty_sold_mean'] = (\n",
    "    df_orders_daily\n",
    "    .groupby('itemID')['qty_sold']\n",
    "    .transform(lambda x: x.shift(1).rolling(window=7).mean())\n",
    ")\n",
    "\n",
    "df_orders_daily['rolling_qty_sold_std'] = (\n",
    "    df_orders_daily\n",
    "    .groupby('itemID')['qty_sold']\n",
    "    .transform(lambda x: x.shift(1).rolling(window=7).std())\n",
    ")\n",
    "\n",
    "df_orders_daily['rolling_qty_sold_median'] = (df_orders_daily\n",
    "    .groupby('itemID')['qty_sold']\n",
    "    .transform(lambda x: x.shift(1).rolling(window=7).median())\n",
    ")\n",
    "\n",
    "#def median_of_uniques(x):\n",
    "#    return np.median(np.unique(x))\n",
    "\n",
    "#df_orders_daily = df_orders_daily.sort_values(['itemID', 'date'])\n",
    "\n",
    "#df_orders_daily['rolling_qty_sold_median_distincts'] = (\n",
    "#    df_orders_daily\n",
    "#    .groupby('itemID')['qty_sold']\n",
    "#    .transform(lambda x: x.shift(1).rolling(window=7).apply(median_of_uniques, raw=True))\n",
    "#)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a8e72c",
   "metadata": {},
   "source": [
    "### Add lagged variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78d1b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by item and date before shift\n",
    "df_orders_daily = df_orders_daily.sort_values(by=[\"itemID\", \"date\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e717cade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at average daily orders and median daily orders of the items\n",
    "# to help decide how many lags are appropriate\n",
    "\n",
    "# define a function to compute the median of unique values\n",
    "def median_of_unique(x):\n",
    "    return np.median(np.unique(x))\n",
    "\n",
    "# compute average daily orders and median of unique daily orders\n",
    "item_daily_stats = (\n",
    "    df_orders_daily.groupby('itemID')[\"qty_sold\"]\n",
    "    .agg(\n",
    "        avg_daily_orders='mean',\n",
    "        median_daily_orders_unique=lambda x: median_of_unique(x)\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "item_daily_stats[\"avg_daily_orders\"] = item_daily_stats[\"avg_daily_orders\"].round(2)\n",
    "item_daily_stats[\"median_daily_orders_unique\"] = item_daily_stats[\"median_daily_orders_unique\"].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6754ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I tested a couple of options\n",
    "# settled on 1 lag only to be able to drop the rows with missing values  resutlting from the lag\n",
    "# without losing much data\n",
    "# anyways, we would expect that the sale from the day directly before will be most significat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61088e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lagged variables for qty_sold (currently, just 1)\n",
    "\n",
    "lags = [1, 2, 3, 7]\n",
    "for lag in lags:\n",
    "    df_orders_daily[f\"qty_sold_lag{lag}\"] = (\n",
    "        df_orders_daily.groupby(\"itemID\")[\"qty_sold\"].shift(lag)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d3c0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder lag columns right after qty_sold\n",
    "# Get all column names\n",
    "cols = list(df_orders_daily.columns)\n",
    "\n",
    "# Remove lag columns from current position\n",
    "lag_cols = [f\"qty_sold_lag{lag}\" for lag in lags]\n",
    "for col in lag_cols:\n",
    "    cols.remove(col)\n",
    "\n",
    "# Find index of qty_sold\n",
    "qty_idx = cols.index(\"qty_sold\")\n",
    "\n",
    "# Insert lag columns in order after qty_sold\n",
    "for i, col in enumerate(lag_cols):\n",
    "    cols.insert(qty_idx + 1 + i, col)\n",
    "\n",
    "# Reorder DataFrame\n",
    "df_orders_daily = df_orders_daily[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db30e36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many values will remain if rows containing NA are dropped\n",
    "\n",
    "len(df_orders_daily.dropna(subset=[\"qty_sold_lag1\", \"qty_sold_lag2\", \"qty_sold_lag3\", \"qty_sold_lag7\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93deef1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what % will remain if rows containing NA are dropped\n",
    "\n",
    "round((len(df_orders_daily.dropna(subset=[\"qty_sold_lag1\", \"qty_sold_lag2\", \"qty_sold_lag3\", \"qty_sold_lag7\"]))/len(df_orders_daily))*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd28c71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping rows containing NA\n",
    "\n",
    "df_orders_daily.dropna(subset=[\"qty_sold_lag1\", \"qty_sold_lag2\", \"qty_sold_lag3\", \"qty_sold_lag7\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e989546f",
   "metadata": {},
   "source": [
    "### Add 2 noise columns for significance testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9140fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# think about whether we want to put some borders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473436ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(22)  # for reproducibility\n",
    "\n",
    "df_orders_daily[\"random_noise1\"] = np.random.normal(0, 1, len(df_orders_daily))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db45037",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(66)  # for reproducibility\n",
    "\n",
    "df_orders_daily[\"random_noise2\"] = np.random.normal(0, 1, len(df_orders_daily))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7b577f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some more rounding\n",
    "\n",
    "df_orders_daily[\"sales_value\"] = df_orders_daily[\"sales_value\"].round(2)\n",
    "df_orders_daily[\"random_noise1\"] = df_orders_daily[\"random_noise1\"].round(2)\n",
    "df_orders_daily[\"random_noise2\"] = df_orders_daily[\"random_noise2\"].round(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
