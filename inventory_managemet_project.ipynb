{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ebb0749",
   "metadata": {},
   "source": [
    "# Inventory management project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbb4e04",
   "metadata": {},
   "source": [
    "## Data exploration and preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37e3e88",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30932e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2836fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "df_infos = pd.read_csv(\"infos.csv\", sep = \"|\")\n",
    "df_items = pd.read_csv(\"items.csv\", sep = \"|\")\n",
    "df_orders_orig = pd.read_csv(\"orders.csv.zip\", sep = \"|\", compression=\"zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0244298f",
   "metadata": {},
   "source": [
    "### Train and Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5360dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix datetime format for transaction time\n",
    "\n",
    "df_orders_orig['time'] = pd.to_datetime(df_orders_orig['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40f4a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_orders_orig[\"itemID\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e536694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the split date\n",
    "split_date = pd.to_datetime(\"08.06.2018\", format=\"%d.%m.%Y\")\n",
    "\n",
    "# Split into train and test\n",
    "df_test = df_orders_orig[df_orders_orig['time'] > split_date]\n",
    "df_orders = df_orders_orig[df_orders_orig['time'] <= split_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c019c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we saw that we lose around 1000 items with the train-test split, because they have orders only in the last 3 weeks\n",
    "# so we check their price x quantity (revenue) - what % it is of the total revenue\n",
    "# to see if we lose a lot with this cropping and decide how to continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5080ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"revenue\"] = df_test[\"order\"] * df_test[\"salesPrice\"]\n",
    "df_orders_orig[\"revenue\"] = df_orders_orig[\"order\"] * df_orders_orig[\"salesPrice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c99c3fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_items = pd.DataFrame(df_test[~df_test[\"itemID\"].isin(df_orders[\"itemID\"])][\"itemID\"].unique(), columns = [\"itemID\"])\n",
    "missing_items[\"will_be_lost\"] = \"yes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "196881c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_orig = df_orders_orig.merge(missing_items, how = \"left\", on = \"itemID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60f7d2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_orig[\"revenue\"] = df_orders_orig[\"salesPrice\"] * df_orders_orig[\"order\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667cd0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "round((df_orders_orig[df_orders_orig[\"will_be_lost\"] == \"yes\"][\"revenue\"].sum()/df_orders_orig[\"revenue\"].sum())*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54581d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# they account for 5.09% of the total revenue of the historical 6-month data we have\n",
    "# it is low enough, we continue like this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5524485d",
   "metadata": {},
   "source": [
    "### df_orders preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebff7530",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98b7d06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add one column which is only with the date, no time\n",
    "\n",
    "df_orders['date'] = df_orders['time'].dt.date\n",
    "df_orders['date'] = pd.to_datetime(df_orders['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fef63620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there seem to be 0 price transactions\n",
    "\n",
    "len(df_orders[df_orders[\"salesPrice\"]==0])/len(df_orders)*100\n",
    "\n",
    "# they are 0.02% of all transactions, it is best to delete them instead of thinking how to handle them\n",
    "\n",
    "df_orders = df_orders[df_orders[\"salesPrice\"]!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abf6da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do all items have an order in the period we have?\n",
    "\n",
    "round(df_orders[\"itemID\"].nunique()/len(df_items)*100, 2)\n",
    "\n",
    "# 94.05% of all items have an order in the period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "149e679c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# which transactions are performed on a discounted price?\n",
    "# we assume the following: a transaction is marked as having a promotion if, for the same item, somewhere else in the table there is\n",
    "# another transaction performed on a lower price\n",
    "\n",
    "# Step 1: Get the maximum price per itemID\n",
    "max_price_per_item = df_orders.groupby('itemID')['salesPrice'].transform('max')\n",
    "\n",
    "# Step 2: Compare each row's price to the max price for that item\n",
    "df_orders['promotion'] = df_orders['salesPrice'] < max_price_per_item\n",
    "\n",
    "# Step 3: Convert boolean to \"yes\"/\"no\"\n",
    "df_orders['promotion'] = df_orders['promotion'].map({True: 1, False: 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1874c69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this data frame will be aggregared on day level for final use\n",
    "# so it is best if we continue the transformation in the aggregated version\n",
    "# but before aggregation, we should check what % of items have been sold on a different price in the same day\n",
    "\n",
    "price_variations = (\n",
    "    df_orders\n",
    "    .assign(date=df_orders['time'].dt.date)\n",
    "    .groupby(['itemID', 'date'])['salesPrice']\n",
    "    .nunique()\n",
    "    .reset_index(name='unique_price_count')\n",
    ")\n",
    "\n",
    "\n",
    "# Filter where price count > 1 (i.e., same item sold at multiple prices)\n",
    "price_variations[price_variations['unique_price_count'] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a58915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# % of such cases from all items\n",
    "\n",
    "((price_variations[price_variations['unique_price_count'] > 1]['unique_price_count'].count())/len(df_items))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c868af23",
   "metadata": {},
   "source": [
    "### df_infos preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c28ce1e",
   "metadata": {},
   "source": [
    "#### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972e7526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in df_infos column promotion there are cells with more than one date, separated by a comma\n",
    "# how many such are there?\n",
    "\n",
    "(df_infos[\"promotion\"].str.len() > 10).sum()\n",
    "\n",
    "# 190\n",
    "# I leave it as text for now, we should handle it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea27e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_infos[\"promotion\"][df_infos[\"promotion\"].str.len() > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95580eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# does df_infos, containing the promotions, contain unique item IDs or are they duplicated?\n",
    "# I expect them to be unique\n",
    "\n",
    "len(df_infos[\"itemID\"]) == len(df_items)\n",
    "df_infos[\"itemID\"].value_counts().max() == 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23756374",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_infos[\"itemID\"].isin(df_items[\"itemID\"]).count() == len(df_infos[\"itemID\"])\n",
    "\n",
    "# it contains a row for each itemID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d81322",
   "metadata": {},
   "source": [
    "#### Deriving discounts for the simulation period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e001db37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders[\"maxPrice\"] = df_orders.groupby(\"itemID\")[\"salesPrice\"].transform(\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4d802ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how do discounts vary\n",
    "\n",
    "df_orders[\"discountAmount\"] = round(df_orders[\"maxPrice\"] - df_orders[\"salesPrice\"], 2)\n",
    "\n",
    "df_orders[\"discountPerc\"] = round(df_orders[\"discountAmount\"]/df_orders[\"maxPrice\"], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88c39d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max and min % discount\n",
    "\n",
    "print(max(df_orders[\"discountPerc\"]), min(df_orders[\"discountPerc\"][df_orders[\"discountPerc\"] != 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757d1c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the histogram and get the bars\n",
    "ax = df_orders[\"discountPerc\"].plot(kind=\"hist\", bins=10, edgecolor='black')\n",
    "\n",
    "# Add value labels on top of each bar\n",
    "for patch in ax.patches:\n",
    "    height = patch.get_height()\n",
    "    if height > 0:\n",
    "        ax.annotate(f'{int(height)}', \n",
    "                    xy=(patch.get_x() + patch.get_width() / 2, height), \n",
    "                    xytext=(0, 5),  # offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "plt.xlabel(\"Discount Percentage\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of Discount Percentage\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e788681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of observations\n",
    "total = len(df_orders[\"discountPerc\"].dropna())\n",
    "\n",
    "# Plot the histogram as density (normalized)\n",
    "ax = df_orders[\"discountPerc\"].plot(kind=\"hist\", bins=10, edgecolor='black', density=False)\n",
    "\n",
    "# Get the actual bin heights (counts) to calculate percentages\n",
    "counts, bins, patches = plt.hist(df_orders[\"discountPerc\"].dropna(), bins=10, edgecolor='black')\n",
    "\n",
    "# Annotate bars with percentage labels\n",
    "for count, patch in zip(counts, patches):\n",
    "    percentage = 100 * count / total\n",
    "    if count > 0:\n",
    "        plt.annotate(f'{percentage:.1f}%', \n",
    "                     xy=(patch.get_x() + patch.get_width() / 2, count), \n",
    "                     xytext=(0, 5),\n",
    "                     textcoords=\"offset points\",\n",
    "                     ha='center', va='bottom')\n",
    "\n",
    "plt.xlabel(\"Discount Percentage\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Histogram of Discount Percentage (with % labels)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f26eab71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the skewed distribution, for getting an approximate discount percentage per item\n",
    "# it would be better to use the median instead of the mean\n",
    "# adding column for discounted price to table df_infos = simulation price - median discount for item\n",
    "# Start with itemID column\n",
    "df_discount_stats = df_items[[\"itemID\"]].copy()\n",
    "\n",
    "# Filter out rows where discountAmount is 0\n",
    "df_nonzero_discounts = df_orders[df_orders[\"discountAmount\"] != 0].copy()\n",
    "\n",
    "# Drop duplicates to keep only unique discount percentages per item\n",
    "unique_discounts = df_nonzero_discounts.drop_duplicates(subset=[\"itemID\", \"discountPerc\"])\n",
    "\n",
    "# Now compute the median of these unique values per item\n",
    "median_discounts = (\n",
    "    unique_discounts\n",
    "    .groupby(\"itemID\")[\"discountPerc\"]\n",
    "    .median()\n",
    "    .round(2)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"discountPerc\": \"medianDiscPerc\"})\n",
    ")\n",
    "\n",
    "# Merge into df_discount_stats\n",
    "df_discount_stats = df_discount_stats.merge(median_discounts, on=\"itemID\", how=\"left\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "501c1962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding column for discounted price to table df_infos = simulation price - median discount for item\n",
    "\n",
    "df_infos = df_infos.merge(df_discount_stats[['itemID', 'medianDiscPerc']], on='itemID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1963db7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding column for discounted price to table df_infos = simulation price - median discount for item\n",
    "\n",
    "df_infos[\"discountedPrice\"] = np.where(\n",
    "    df_infos[\"promotion\"].notna(),\n",
    "    round(df_infos[\"simulationPrice\"] * (1 - df_infos[\"medianDiscPerc\"]), 2),\n",
    "    np.nan  # or just leave it to default if you prefer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9afc1dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfinished - we have to use some mean based on similar items to derive median discount % for items which will have\n",
    "# a promotion in the simulation period but have not had a discount in the historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0d3ea76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding also min price per item in the orders data frame for completion\n",
    "\n",
    "df_orders[\"minPrice\"] = df_orders.groupby(\"itemID\")[\"salesPrice\"].transform(\"min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4570e5cb",
   "metadata": {},
   "source": [
    "#### Quickly check relation - qty sold and promotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a2785884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# promo tests\n",
    "\n",
    "# Step 1: Sum quantity per itemID, date, and promotion (daily sales)\n",
    "daily_qty = (\n",
    "    df_orders\n",
    "    .groupby(['itemID', 'date', 'promotion'])['order']\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Step 2: Aggregate by itemID and promotion: \n",
    "# total quantity sold (sum of daily sums)\n",
    "# count of days with sales (number of unique days)\n",
    "agg = daily_qty.groupby(['itemID', 'promotion']).agg(\n",
    "    total_qty=('order', 'sum'),\n",
    "    count_days=('date', 'nunique')\n",
    ").unstack(fill_value=0)\n",
    "\n",
    "# Step 3: Build the final DataFrame safely extracting promo/no promo columns\n",
    "summary = pd.DataFrame({\n",
    "    'QTY_no_promo': agg['total_qty'].get(0, pd.Series(0)),\n",
    "    'QTY_promo': agg['total_qty'].get(1, pd.Series(0)),\n",
    "    'count_days_no_promo': agg['count_days'].get(0, pd.Series(0)),\n",
    "    'count_days_promo': agg['count_days'].get(1, pd.Series(0))\n",
    "}).reset_index()\n",
    "\n",
    "# Step 4: Calculate average quantity per day (handle division by zero)\n",
    "summary['QTY_no_promo_per_day'] = summary.apply(\n",
    "    lambda r: r['QTY_no_promo'] / r['count_days_no_promo'] if r['count_days_no_promo'] > 0 else 0,\n",
    "    axis=1\n",
    ")\n",
    "summary['QTY_promo_per_day'] = summary.apply(\n",
    "    lambda r: r['QTY_promo'] / r['count_days_promo'] if r['count_days_promo'] > 0 else 0,\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d1a439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# promo tests\n",
    "\n",
    "len(summary[summary[\"QTY_promo_per_day\"] > summary[\"QTY_no_promo_per_day\"]])/len(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abac634a",
   "metadata": {},
   "source": [
    "## Aggregate orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "30b73b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate df_orders on a daily basis\n",
    "# sum of QTY\n",
    "# average of price (or median?)?\n",
    "# promotion - if 1 is present, then 1 (had at least 1 promotion in that day)\n",
    "# median discount %?\n",
    "# median discount amount?\n",
    "\n",
    "# to make a desicion wether to use mean of median for price, discount amount, discount perc\n",
    "# we have to look at the distribution of the prices for some items\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e273a6",
   "metadata": {},
   "source": [
    "### Checking price per item distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a469294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a column with item_prices_count to df_orders\n",
    "\n",
    "df_orders[\"item_prices_count\"] = df_orders.groupby(\"itemID\")[\"salesPrice\"].transform(lambda x: x.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "92d46af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting a random sample where the item has price discount of > 0.79, meaning there might\n",
    "# be great price variations of the item\n",
    "\n",
    "df_sample = df_orders[[\"itemID\"]][df_orders[\"discountPerc\"] > 0.79]\n",
    "\n",
    "df_sample = df_sample.sample(n=50, random_state=222)\n",
    "\n",
    "df_sample = df_sample.sort_values(by=\"itemID\", ascending=True)\n",
    "\n",
    "df_sample = df_sample.merge(df_orders, how=\"left\", on=\"itemID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f9cc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing \n",
    "\n",
    "# Unique items\n",
    "item_ids = df_sample['itemID'].unique()\n",
    "\n",
    "# Set up the grid\n",
    "rows, cols = 10, 5\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(20, 15), sharex=False, sharey=False)\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot histogram for each item\n",
    "for i, item_id in enumerate(item_ids):\n",
    "    ax = axes[i]\n",
    "    item_prices = df_sample[df_sample['itemID'] == item_id]['salesPrice']\n",
    "\n",
    "    ax.hist(item_prices, bins=10, color='skyblue', edgecolor='black')\n",
    "    ax.set_title(f'Item {item_id}', fontsize=8)\n",
    "    ax.tick_params(labelsize=6)\n",
    "    ax.grid(True)\n",
    "\n",
    "# Hide unused subplots\n",
    "for j in range(len(item_ids), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "df63ca64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it would be useful to see another check to make aa decision\n",
    "# see the top 10 items for which the mean and median are the most different and\n",
    "# see which one makes more sence for us\n",
    "\n",
    "# Group by itemID and compute mean and median\n",
    "price_stats = df_orders.groupby('itemID')['salesPrice'].agg(\n",
    "    mean_price='mean',\n",
    "    median_price='median'\n",
    ").reset_index()\n",
    "\n",
    "# Compute absolute difference\n",
    "price_stats['abs_diff'] = (price_stats['mean_price'] - price_stats['median_price']).abs()\n",
    "\n",
    "# Compute absolute percentage difference relative to median\n",
    "price_stats['abs_perc_diff'] = (price_stats['abs_diff'] / price_stats['median_price']).abs() * 100\n",
    "\n",
    "# Round numerical columns\n",
    "price_stats[['mean_price', 'median_price', 'abs_diff', 'abs_perc_diff']] = price_stats[\n",
    "    ['mean_price', 'median_price', 'abs_diff', 'abs_perc_diff']].round(2)\n",
    "\n",
    "# Sort by absolute percentage difference descending\n",
    "price_stats = price_stats.sort_values(by='abs_perc_diff', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "256597e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now look at histograms of top 50 of items with most % difference of mean and median\n",
    "\n",
    "df_sample = price_stats.sort_values(by='abs_perc_diff', ascending=False).head(50)[[\"itemID\"]]\n",
    "\n",
    "df_sample = df_sample.sample(n=50, random_state=222)\n",
    "\n",
    "df_sample = df_sample.sort_values(by=\"itemID\", ascending=True)\n",
    "\n",
    "df_sample = df_sample.merge(df_orders, how=\"left\", on=\"itemID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcfd246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing \n",
    "\n",
    "# Unique items\n",
    "item_ids = df_sample['itemID'].unique()\n",
    "\n",
    "# Set up the grid\n",
    "rows, cols = 10, 5\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(20, 15), sharex=False, sharey=False)\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot histogram for each item\n",
    "for i, item_id in enumerate(item_ids):\n",
    "    ax = axes[i]\n",
    "    item_prices = df_sample[df_sample['itemID'] == item_id]['salesPrice']\n",
    "\n",
    "    ax.hist(item_prices, bins=10, color='skyblue', edgecolor='black')\n",
    "    ax.set_title(f'Item {item_id}', fontsize=8)\n",
    "    ax.tick_params(labelsize=6)\n",
    "    ax.grid(True)\n",
    "\n",
    "# Hide unused subplots\n",
    "for j in range(len(item_ids), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e719f780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is better to use mean because the extreme values are not a one-case accidental thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1415bd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate using mean (average), but specifically weighted average, to gain price per each\n",
    "# weighted average will take into account how accidental the outlier prices were"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e49f5b",
   "metadata": {},
   "source": [
    "### Aggregate orders on day level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ff1227cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to get weighted average price per each for the items\n",
    "# first we need to add a column to df_orders\n",
    "# with order value = qty * price\n",
    "\n",
    "df_orders[\"orderValue\"] = df_orders[\"order\"] * df_orders[\"salesPrice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eaa9740d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregating on day level\n",
    "\n",
    "df_orders_daily = df_orders.groupby(['date', 'itemID']).agg(\n",
    "    qty_sold=('order', 'sum'),\n",
    "    sales_value=('orderValue', 'sum'),\n",
    "    promotion=('promotion', 'max'),  # If any transaction had promotion == 1, result will be 1\n",
    "    maxItemPrice=('maxPrice', 'max'), # doesn't matter min or max - its the same value for all transactions with the same item\n",
    "    minItemPrice=('minPrice', 'max') # doesn't matter min or max - its the same value for all transactions with the same item\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1cc99583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deriving price per each for the items\n",
    "# checkp change to last mode\n",
    "\n",
    "df_orders_daily[\"TotalItemQty\"] = df_orders_daily.groupby(\"itemID\")[\"qty_sold\"].transform(\"sum\")\n",
    "df_orders_daily[\"TotalItemValue\"] = df_orders_daily.groupby(\"itemID\")[\"sales_value\"].transform(\"sum\")\n",
    "\n",
    "df_orders_daily[\"PricePerEach\"] = round(df_orders_daily[\"TotalItemValue\"] / df_orders_daily[\"TotalItemQty\"], 2)\n",
    "\n",
    "df_orders_daily = df_orders_daily.drop(['TotalItemQty', 'TotalItemValue'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6f8ad04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deriving price per each for the items, but specific price for that day\n",
    "\n",
    "df_orders_daily[\"DailyItemQty\"] = df_orders_daily.groupby([\"itemID\", \"date\"])[\"qty_sold\"].transform(\"sum\")\n",
    "df_orders_daily[\"DailyItemValue\"] = df_orders_daily.groupby([\"itemID\", \"date\"])[\"sales_value\"].transform(\"sum\")\n",
    "\n",
    "df_orders_daily[\"PricePerEachToday\"] = round(df_orders_daily[\"DailyItemValue\"] / df_orders_daily[\"DailyItemQty\"], 2)\n",
    "\n",
    "df_orders_daily = df_orders_daily.drop(['DailyItemQty', 'DailyItemValue'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e63381aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add median discout for the items\n",
    "# we have it currently for each item in df_infos\n",
    "\n",
    "# Select only itemID and medianDiscount from df_infos and merge on itemID\n",
    "df_orders_daily = df_orders_daily.merge(\n",
    "    df_infos[[\"itemID\", \"medianDiscPerc\"]],\n",
    "    how=\"left\",\n",
    "    on=\"itemID\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63d904b",
   "metadata": {},
   "source": [
    "## Add features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c3310a",
   "metadata": {},
   "source": [
    "#### Complete main DF with missing date + item combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c3279e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# completing our main data frame with all missing day+item combinations\n",
    "# for them qty_sold = 0, sales_value = 0, promotion = 0, maxItemPrice = maxItemPrice, minItemPrice = minItemPrice\n",
    "# PricePerEach = mean PricePerEach for that item, medianDiscPerc = medianDiscPerc\n",
    "\n",
    "new_df = pd.DataFrame(df_orders_daily[\"itemID\"].unique(), columns = [\"itemID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "639216b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create date range\n",
    "date_range = pd.date_range(start='2018-01-01', end='2018-06-07', freq='D')\n",
    "\n",
    "# Create DataFrame\n",
    "df_dates = pd.DataFrame({'date': date_range})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c3701829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a dummy key to both DataFrames\n",
    "df_dates[\"key\"] = 1\n",
    "new_df[\"key\"] = 1\n",
    "\n",
    "# Perform cross join\n",
    "new_df = pd.merge(df_dates, new_df, on=\"key\").drop(\"key\", axis=1)\n",
    "\n",
    "# Sort by date and then itemID\n",
    "new_df = new_df.sort_values(by=[\"date\", \"itemID\"]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "50c23aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_daily = new_df.merge(df_orders_daily, how=\"left\", on=[\"date\", \"itemID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fb56a758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for them qty_sold = 0, sales_value = 0, promotion = 0\n",
    "\n",
    "df_orders_daily[[\"qty_sold\", \"sales_value\", \"promotion\"]] = df_orders_daily[[\"qty_sold\", \"sales_value\", \"promotion\"]].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "971cb53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maxItemPrice = maxItemPrice, minItemPrice = minItemPrice, PricePerEach = mean PricePerEach for that item, medianDiscPerc = medianDiscPerc\n",
    "\n",
    "# Fill missing maxItemPrice with the max per itemID\n",
    "df_orders_daily[\"maxItemPrice\"] = df_orders_daily.groupby(\"itemID\")[\"maxItemPrice\"].transform(lambda x: x.fillna(x.max()))\n",
    "\n",
    "# Fill missing minItemPrice with the max per itemID\n",
    "df_orders_daily[\"minItemPrice\"] = df_orders_daily.groupby(\"itemID\")[\"minItemPrice\"].transform(lambda x: x.fillna(x.max()))\n",
    "\n",
    "# Fill missing PricePerEach with the max per itemID\n",
    "df_orders_daily[\"PricePerEach\"] = df_orders_daily.groupby(\"itemID\")[\"PricePerEach\"].transform(lambda x: x.fillna(x.max()))\n",
    "\n",
    "# Fill missing PricePerEachToday with the mean per itemID\n",
    "df_orders_daily[\"PricePerEachToday\"] = df_orders_daily.groupby(\"itemID\")[\"PricePerEachToday\"].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "df_orders_daily[\"PricePerEachToday\"] = df_orders_daily[\"PricePerEachToday\"].round(2)\n",
    "\n",
    "# Fill missing medianDiscPerc with the max per itemID\n",
    "df_orders_daily[\"medianDiscPerc\"] = df_orders_daily.groupby(\"itemID\")[\"medianDiscPerc\"].transform(lambda x: x.fillna(x.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d895f1b",
   "metadata": {},
   "source": [
    "#### Add masterdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "14b90805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# include masterdata\n",
    "\n",
    "df_orders_daily = df_orders_daily.merge(df_items, how=\"left\", on=\"itemID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d660dc7a",
   "metadata": {},
   "source": [
    "#### Add date features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "eca058ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add date features\n",
    "df_orders_daily[\"weekDay\"] = df_orders_daily[\"date\"].dt.weekday + 1\n",
    "df_orders_daily[\"day\"] = df_orders_daily[\"date\"].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "15b54baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_week_of_month(date):\n",
    "    # First day of the month\n",
    "    first_day = date.replace(day=1)\n",
    "    \n",
    "    # Find the day of the week the first day lands on (Monday=0, Sunday=6)\n",
    "    first_day_weekday = first_day.weekday()\n",
    "    \n",
    "    # Calendar row index = (day of month + offset from Monday) // 7 + 1\n",
    "    return ((date.day + first_day_weekday - 1) // 7) + 1\n",
    "\n",
    "# Apply the function to create the column\n",
    "df_orders_daily['weekOfMonth'] = df_orders_daily['date'].apply(get_week_of_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0258b357",
   "metadata": {},
   "source": [
    "#### Add lagged variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "851272b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by item and date before shift\n",
    "df_orders_daily = df_orders_daily.sort_values(by=[\"itemID\", \"date\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b0c664e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at average daily orders and median daily orders of the items\n",
    "# to help decide how many lags are appropriate\n",
    "\n",
    "# define a function to compute the median of unique values\n",
    "def median_of_unique(x):\n",
    "    return np.median(np.unique(x))\n",
    "\n",
    "# compute average daily orders and median of unique daily orders\n",
    "item_daily_stats = (\n",
    "    df_orders_daily.groupby('itemID')[\"qty_sold\"]\n",
    "    .agg(\n",
    "        avg_daily_orders='mean',\n",
    "        median_daily_orders_unique=lambda x: median_of_unique(x)\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "item_daily_stats[\"avg_daily_orders\"] = item_daily_stats[\"avg_daily_orders\"].round(2)\n",
    "item_daily_stats[\"median_daily_orders_unique\"] = item_daily_stats[\"median_daily_orders_unique\"].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4bccae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I tested a couple of options\n",
    "# settled on 1 lag only to be able to drop the rows with missing values  resutlting from the lag\n",
    "# without losing much data\n",
    "# anyways, we would expect that the sale from the day directly before will be most significat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "473a8bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lagged variables for qty_sold (currently, just 1)\n",
    "\n",
    "lags = [1, 2, 3, 7]\n",
    "for lag in lags:\n",
    "    df_orders_daily[f\"qty_sold_lag{lag}\"] = (\n",
    "        df_orders_daily.groupby(\"itemID\")[\"qty_sold\"].shift(lag)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "89ef19a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder lag columns right after qty_sold\n",
    "# Get all column names\n",
    "cols = list(df_orders_daily.columns)\n",
    "\n",
    "# Remove lag columns from current position\n",
    "lag_cols = [f\"qty_sold_lag{lag}\" for lag in lags]\n",
    "for col in lag_cols:\n",
    "    cols.remove(col)\n",
    "\n",
    "# Find index of qty_sold\n",
    "qty_idx = cols.index(\"qty_sold\")\n",
    "\n",
    "# Insert lag columns in order after qty_sold\n",
    "for i, col in enumerate(lag_cols):\n",
    "    cols.insert(qty_idx + 1 + i, col)\n",
    "\n",
    "# Reorder DataFrame\n",
    "df_orders_daily = df_orders_daily[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a286a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many values will remain if rows containing NA are dropped\n",
    "\n",
    "len(df_orders_daily.dropna(subset=[\"qty_sold_lag1\", \"qty_sold_lag2\", \"qty_sold_lag3\", \"qty_sold_lag7\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ad4f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what % will remain if rows containing NA are dropped\n",
    "\n",
    "round((len(df_orders_daily.dropna(subset=[\"qty_sold_lag1\", \"qty_sold_lag2\", \"qty_sold_lag3\", \"qty_sold_lag7\"]))/len(df_orders_daily))*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e83f7ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping rows containing NA\n",
    "\n",
    "df_orders_daily.dropna(subset=[\"qty_sold_lag1\", \"qty_sold_lag2\", \"qty_sold_lag3\", \"qty_sold_lag7\"], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e59b29",
   "metadata": {},
   "source": [
    "### Add 2 noise columns for significance testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "94e3bccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# think about whether we want to put some borders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "511a337c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(22)  # for reproducibility\n",
    "\n",
    "df_orders_daily[\"random_noise1\"] = np.random.normal(0, 1, len(df_orders_daily))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cd8b24df",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(66)  # for reproducibility\n",
    "\n",
    "df_orders_daily[\"random_noise2\"] = np.random.normal(0, 1, len(df_orders_daily))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15acd915",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cc625b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_harmonics(data, num_harmonics=10, return_wave=None):\n",
    "    all_coefs = np.fft.fft(data)\n",
    "    coeffs = []\n",
    "    nh = return_wave + 1 if return_wave is not None else num_harmonics\n",
    "    for i in range(1, nh + 1):\n",
    "        coeffs.append(np.zeros(len(all_coefs), dtype=complex))\n",
    "        coeffs[-1][i] = all_coefs[i]\n",
    "        coeffs[-1][-i] = all_coefs[-i]\n",
    "\n",
    "    if return_wave is not None:\n",
    "        rc = np.zeros(len(all_coefs), dtype=complex) + coeffs[return_wave]\n",
    "        rc = np.fft.ifft(rc).real\n",
    "        return rc\n",
    "\n",
    "    reconstructed_coeffs = np.zeros(len(all_coefs), dtype=complex)\n",
    "    for i in range(num_harmonics):\n",
    "        reconstructed_coeffs += coeffs[i]\n",
    "    reconstructed_signal = np.fft.ifft(reconstructed_coeffs).real\n",
    "    reconstructed_signal += data.mean()\n",
    "    return reconstructed_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "686c85e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    df_orders_daily[f'harmonic_{i}'] = df_orders_daily.groupby(by=\"itemID\")[\"qty_sold\"].transform(lambda c: get_harmonics(c, return_wave=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2d616cb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'itemID', 'qty_sold', 'qty_sold_lag1', 'qty_sold_lag2',\n",
       "       'qty_sold_lag3', 'qty_sold_lag7', 'sales_value', 'promotion',\n",
       "       'maxItemPrice', 'minItemPrice', 'PricePerEach', 'PricePerEachToday',\n",
       "       'brand', 'manufacturer', 'customerRating', 'category1', 'category2',\n",
       "       'category3', 'recommendedRetailPrice', 'weekDay', 'day', 'weekOfMonth',\n",
       "       'random_noise1', 'random_noise2', 'harmonic_0', 'harmonic_1',\n",
       "       'harmonic_2', 'harmonic_3', 'harmonic_4'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orders_daily.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c05837a",
   "metadata": {},
   "source": [
    "STUFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca563bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "def arimax(ts, ex, order=(1,0, 1)):\n",
    "    model = SARIMAX(ts, exog=ex, order=order)\n",
    "    model_fit = model.fit()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441d524b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, df in df_orders_daily.groupby(\"itemID\"):\n",
    "    if (df['qty_sold'] > 0).sum() > 150:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b5ca22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_daily.drop(columns=['medianDiscPerc'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1b231b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "grr = df_orders_daily[df_orders_daily['itemID'] == 7798]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df60bf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = arimax(grr['qty_sold'], grr.drop(columns=['qty_sold', 'date', 'itemID']))\n",
    "# grr.drop(columns=['qty_sold', 'date', 'itemID']).isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb81d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "grr.apply(lambda x: len(x.unique()), axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "case-automated-inventory-management-execut-tVQubp4F",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
