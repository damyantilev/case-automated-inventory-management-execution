{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ebb0749",
   "metadata": {},
   "source": [
    "# Inventory management project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbb4e04",
   "metadata": {},
   "source": [
    "## Data exploration and preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37e3e88",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30932e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2836fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "df_infos = pd.read_csv(\"infos.csv\", sep = \"|\")\n",
    "df_items = pd.read_csv(\"items.csv\", sep = \"|\")\n",
    "df_orders = pd.read_csv(\"orders.csv.zip\", sep = \"|\", compression=\"zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0244298f",
   "metadata": {},
   "source": [
    "### Train and Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5360dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix datetime format for transaction time\n",
    "\n",
    "df_orders['time'] = pd.to_datetime(df_orders['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40f4a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_orders[\"itemID\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e536694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the split date\n",
    "split_date = pd.to_datetime(\"08.06.2018\", format=\"%d.%m.%Y\")\n",
    "\n",
    "df_orders['time'] = pd.to_datetime(df_orders['time'])  # ensure it's datetime\n",
    "split_date_only = split_date.date()\n",
    "\n",
    "df_test = df_orders[df_orders['time'].dt.date > split_date_only]\n",
    "df_train = df_orders[df_orders['time'].dt.date <= split_date_only]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c019c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we saw that we lose around 1000 items with the train-test split, because they have orders only in the last 3 weeks\n",
    "# so we check their price x quantity (revenue) - what % it is of the total revenue\n",
    "# to see if we lose a lot with this cropping and decide how to continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5080ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"revenue\"] = df_test[\"order\"] * df_test[\"salesPrice\"]\n",
    "df_orders[\"revenue\"] = df_orders[\"order\"] * df_orders[\"salesPrice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99c3fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_items = pd.DataFrame(df_test[~df_test[\"itemID\"].isin(df_train[\"itemID\"])][\"itemID\"].unique(), columns = [\"itemID\"])\n",
    "missing_items[\"will_be_lost\"] = \"yes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196881c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders = df_orders.merge(missing_items, how = \"left\", on = \"itemID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f7d2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders[\"revenue\"] = df_orders[\"salesPrice\"] * df_orders[\"order\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667cd0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "round((df_orders[df_orders[\"will_be_lost\"] == \"yes\"][\"revenue\"].sum()/df_orders[\"revenue\"].sum())*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54581d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# they account for 5.09% of the total revenue of the historical 6-month data we have\n",
    "# it is low enough, we continue like this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5524485d",
   "metadata": {},
   "source": [
    "### df_train preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebff7530",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b7d06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add one column which is only with the date, no time\n",
    "\n",
    "# train\n",
    "df_train['date'] = df_train['time'].dt.date\n",
    "df_train['date'] = pd.to_datetime(df_train['date'])\n",
    "\n",
    "# test\n",
    "df_test['date'] = df_test['time'].dt.date\n",
    "df_test['date'] = pd.to_datetime(df_test['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef63620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there seem to be 0 price transactions\n",
    "\n",
    "len(df_train[df_train[\"salesPrice\"]==0])/len(df_train)*100\n",
    "\n",
    "# they are 0.02% of all transactions, it is best to delete them instead of thinking how to handle them\n",
    "\n",
    "df_train = df_train[df_train[\"salesPrice\"]!=0]\n",
    "\n",
    "# and now for test\n",
    "\n",
    "df_test = df_test[df_test[\"salesPrice\"]!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abf6da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do all items have an order in the period we have?\n",
    "\n",
    "round(df_train[\"itemID\"].nunique()/len(df_items)*100, 2)\n",
    "\n",
    "# 94.05% of all items have an order in the period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149e679c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# which transactions are performed on a discounted price?\n",
    "# we assume the following: a transaction is marked as having a promotion if, for the same item, somewhere else in the table there is\n",
    "# another transaction performed on a lower price\n",
    "\n",
    "# Step 1: Get the maximum price per itemID\n",
    "max_price_per_item = df_train.groupby('itemID')['salesPrice'].transform('max')\n",
    "\n",
    "# Step 2: Compare each row's price to the max price for that item\n",
    "df_train['promotion'] = df_train['salesPrice'] < max_price_per_item\n",
    "\n",
    "# Step 3: Convert boolean to \"yes\"/\"no\"\n",
    "df_train['promotion'] = df_train['promotion'].map({True: 1, False: 0})\n",
    "\n",
    "\n",
    "# and now for test\n",
    "\n",
    "# Step 1: Get the maximum price per itemID\n",
    "max_price_per_item = df_test.groupby('itemID')['salesPrice'].transform('max')\n",
    "\n",
    "# Step 2: Compare each row's price to the max price for that item\n",
    "df_test['promotion'] = df_test['salesPrice'] < max_price_per_item\n",
    "\n",
    "# Step 3: Convert boolean to \"yes\"/\"no\"\n",
    "df_test['promotion'] = df_test['promotion'].map({True: 1, False: 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1874c69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this data frame will be aggregared on day level for final use\n",
    "# so it is best if we continue the transformation in the aggregated version\n",
    "# but before aggregation, we should check what % of items have been sold on a different price in the same day\n",
    "\n",
    "price_variations = (\n",
    "    df_train\n",
    "    .assign(date=df_train['time'].dt.date)\n",
    "    .groupby(['itemID', 'date'])['salesPrice']\n",
    "    .nunique()\n",
    "    .reset_index(name='unique_price_count')\n",
    ")\n",
    "\n",
    "\n",
    "# Filter where price count > 1 (i.e., same item sold at multiple prices)\n",
    "price_variations[price_variations['unique_price_count'] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a58915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# % of such cases from all items\n",
    "\n",
    "((price_variations[price_variations['unique_price_count'] > 1]['unique_price_count'].count())/len(df_items))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c868af23",
   "metadata": {},
   "source": [
    "### df_infos preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c28ce1e",
   "metadata": {},
   "source": [
    "#### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972e7526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in df_infos column promotion there are cells with more than one date, separated by a comma\n",
    "# how many such are there?\n",
    "\n",
    "(df_infos[\"promotion\"].str.len() > 10).sum()\n",
    "\n",
    "# 190\n",
    "# I leave it as text for now, we should handle it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea27e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_infos[\"promotion\"][df_infos[\"promotion\"].str.len() > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95580eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# does df_infos, containing the promotions, contain unique item IDs or are they duplicated?\n",
    "# I expect them to be unique\n",
    "\n",
    "len(df_infos[\"itemID\"]) == len(df_items)\n",
    "df_infos[\"itemID\"].value_counts().max() == 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23756374",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_infos[\"itemID\"].isin(df_items[\"itemID\"]).count() == len(df_infos[\"itemID\"])\n",
    "\n",
    "# it contains a row for each itemID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d81322",
   "metadata": {},
   "source": [
    "#### Deriving discounts for the simulation period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e001db37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"maxPrice\"] = df_train.groupby(\"itemID\")[\"salesPrice\"].transform(\"max\")\n",
    "\n",
    "# and for test\n",
    "\n",
    "df_test[\"maxPrice\"] = df_test.groupby(\"itemID\")[\"salesPrice\"].transform(\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d802ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deriving discounts\n",
    "\n",
    "df_train[\"discountAmount\"] = round(df_train[\"maxPrice\"] - df_train[\"salesPrice\"], 2)\n",
    "\n",
    "df_train[\"discountPerc\"] = round(df_train[\"discountAmount\"]/df_train[\"maxPrice\"], 2)\n",
    "\n",
    "# and for test\n",
    "\n",
    "df_test[\"discountAmount\"] = round(df_test[\"maxPrice\"] - df_test[\"salesPrice\"], 2)\n",
    "\n",
    "df_test[\"discountPerc\"] = round(df_test[\"discountAmount\"]/df_test[\"maxPrice\"], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88c39d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max and min % discount\n",
    "\n",
    "print(max(df_train[\"discountPerc\"]), min(df_train[\"discountPerc\"][df_train[\"discountPerc\"] != 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757d1c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the histogram and get the bars\n",
    "ax = df_train[\"discountPerc\"].plot(kind=\"hist\", bins=10, edgecolor='black')\n",
    "\n",
    "# Add value labels on top of each bar\n",
    "for patch in ax.patches:\n",
    "    height = patch.get_height()\n",
    "    if height > 0:\n",
    "        ax.annotate(f'{int(height)}', \n",
    "                    xy=(patch.get_x() + patch.get_width() / 2, height), \n",
    "                    xytext=(0, 5),  # offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "plt.xlabel(\"Discount Percentage\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of Discount Percentage\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e788681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of observations\n",
    "total = len(df_train[\"discountPerc\"].dropna())\n",
    "\n",
    "# Plot the histogram as density (normalized)\n",
    "ax = df_train[\"discountPerc\"].plot(kind=\"hist\", bins=10, edgecolor='black', density=False)\n",
    "\n",
    "# Get the actual bin heights (counts) to calculate percentages\n",
    "counts, bins, patches = plt.hist(df_train[\"discountPerc\"].dropna(), bins=10, edgecolor='black')\n",
    "\n",
    "# Annotate bars with percentage labels\n",
    "for count, patch in zip(counts, patches):\n",
    "    percentage = 100 * count / total\n",
    "    if count > 0:\n",
    "        plt.annotate(f'{percentage:.1f}%', \n",
    "                     xy=(patch.get_x() + patch.get_width() / 2, count), \n",
    "                     xytext=(0, 5),\n",
    "                     textcoords=\"offset points\",\n",
    "                     ha='center', va='bottom')\n",
    "\n",
    "plt.xlabel(\"Discount Percentage\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Histogram of Discount Percentage (with % labels)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26eab71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the skewed distribution, for getting an approximate discount percentage per item\n",
    "# it would be better to use the median instead of the mean\n",
    "# adding column for discounted price to table df_infos = simulation price - median discount for item\n",
    "# Start with itemID column\n",
    "df_discount_stats = df_items[[\"itemID\"]].copy()\n",
    "\n",
    "# Filter out rows where discountAmount is 0\n",
    "df_nonzero_discounts = df_train[df_train[\"discountAmount\"] != 0].copy()\n",
    "\n",
    "# Drop duplicates to keep only unique discount percentages per item\n",
    "unique_discounts = df_nonzero_discounts.drop_duplicates(subset=[\"itemID\", \"discountPerc\"])\n",
    "\n",
    "# Now compute the median of these unique values per item\n",
    "median_discounts = (\n",
    "    unique_discounts\n",
    "    .groupby(\"itemID\")[\"discountPerc\"]\n",
    "    .median()\n",
    "    .round(2)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"discountPerc\": \"medianDiscPerc\"})\n",
    ")\n",
    "\n",
    "# Merge into df_discount_stats\n",
    "df_discount_stats = df_discount_stats.merge(median_discounts, on=\"itemID\", how=\"left\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501c1962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding column for discounted price to table df_infos = simulation price - median discount for item\n",
    "\n",
    "df_infos = df_infos.merge(df_discount_stats[['itemID', 'medianDiscPerc']], on='itemID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1963db7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding column for discounted price to table df_infos = simulation price - median discount for item\n",
    "\n",
    "df_infos[\"discountedPrice\"] = np.where(\n",
    "    df_infos[\"promotion\"].notna(),\n",
    "    round(df_infos[\"simulationPrice\"] * (1 - df_infos[\"medianDiscPerc\"]), 2),\n",
    "    np.nan  # or just leave it to default if you prefer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afc1dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfinished - we have to use some mean based on similar items to derive median discount % for items which will have\n",
    "# a promotion in the simulation period but have not had a discount in the historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3ea76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding also min price per item in the orders data frame for completion\n",
    "\n",
    "df_train[\"minPrice\"] = df_train.groupby(\"itemID\")[\"salesPrice\"].transform(\"min\")\n",
    "\n",
    "# and for test\n",
    "\n",
    "df_test[\"minPrice\"] = df_test.groupby(\"itemID\")[\"salesPrice\"].transform(\"min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4570e5cb",
   "metadata": {},
   "source": [
    "#### Quickly check relation - qty sold and promotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2785884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# promo tests\n",
    "\n",
    "# Step 1: Sum quantity per itemID, date, and promotion (daily sales)\n",
    "daily_qty = (\n",
    "    df_train\n",
    "    .groupby(['itemID', 'date', 'promotion'])['order']\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Step 2: Aggregate by itemID and promotion: \n",
    "# total quantity sold (sum of daily sums)\n",
    "# count of days with sales (number of unique days)\n",
    "agg = daily_qty.groupby(['itemID', 'promotion']).agg(\n",
    "    total_qty=('order', 'sum'),\n",
    "    count_days=('date', 'nunique')\n",
    ").unstack(fill_value=0)\n",
    "\n",
    "# Step 3: Build the final DataFrame safely extracting promo/no promo columns\n",
    "summary = pd.DataFrame({\n",
    "    'QTY_no_promo': agg['total_qty'].get(0, pd.Series(0)),\n",
    "    'QTY_promo': agg['total_qty'].get(1, pd.Series(0)),\n",
    "    'count_days_no_promo': agg['count_days'].get(0, pd.Series(0)),\n",
    "    'count_days_promo': agg['count_days'].get(1, pd.Series(0))\n",
    "}).reset_index()\n",
    "\n",
    "# Step 4: Calculate average quantity per day (handle division by zero)\n",
    "summary['QTY_no_promo_per_day'] = summary.apply(\n",
    "    lambda r: r['QTY_no_promo'] / r['count_days_no_promo'] if r['count_days_no_promo'] > 0 else 0,\n",
    "    axis=1\n",
    ")\n",
    "summary['QTY_promo_per_day'] = summary.apply(\n",
    "    lambda r: r['QTY_promo'] / r['count_days_promo'] if r['count_days_promo'] > 0 else 0,\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d1a439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# promo tests\n",
    "\n",
    "len(summary[summary[\"QTY_promo_per_day\"] > summary[\"QTY_no_promo_per_day\"]])/len(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abac634a",
   "metadata": {},
   "source": [
    "## Aggregate orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b73b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate df_train on a daily basis\n",
    "# sum of QTY\n",
    "# average of price (or median?)?\n",
    "# promotion - if 1 is present, then 1 (had at least 1 promotion in that day)\n",
    "# median discount %?\n",
    "# median discount amount?\n",
    "\n",
    "# to make a desicion wether to use mean of median for price, discount amount, discount perc\n",
    "# we have to look at the distribution of the prices for some items\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e273a6",
   "metadata": {},
   "source": [
    "### Checking price per item distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a469294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a column with item_prices_count to df_train\n",
    "\n",
    "df_train[\"item_prices_count\"] = df_train.groupby(\"itemID\")[\"salesPrice\"].transform(lambda x: x.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d46af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting a random sample where the item has price discount of > 0.79, meaning there might\n",
    "# be great price variations of the item\n",
    "\n",
    "df_sample = df_train[[\"itemID\"]][df_train[\"discountPerc\"] > 0.79]\n",
    "\n",
    "df_sample = df_sample.sample(n=50, random_state=222)\n",
    "\n",
    "df_sample = df_sample.sort_values(by=\"itemID\", ascending=True)\n",
    "\n",
    "df_sample = df_sample.merge(df_train, how=\"left\", on=\"itemID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f9cc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing \n",
    "\n",
    "# Unique items\n",
    "item_ids = df_sample['itemID'].unique()\n",
    "\n",
    "# Set up the grid\n",
    "rows, cols = 10, 5\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(20, 15), sharex=False, sharey=False)\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot histogram for each item\n",
    "for i, item_id in enumerate(item_ids):\n",
    "    ax = axes[i]\n",
    "    item_prices = df_sample[df_sample['itemID'] == item_id]['salesPrice']\n",
    "\n",
    "    ax.hist(item_prices, bins=10, color='skyblue', edgecolor='black')\n",
    "    ax.set_title(f'Item {item_id}', fontsize=8)\n",
    "    ax.tick_params(labelsize=6)\n",
    "    ax.grid(True)\n",
    "\n",
    "# Hide unused subplots\n",
    "for j in range(len(item_ids), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df63ca64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it would be useful to see another check to make aa decision\n",
    "# see the top 10 items for which the mean and median are the most different and\n",
    "# see which one makes more sence for us\n",
    "\n",
    "# Group by itemID and compute mean and median\n",
    "price_stats = df_train.groupby('itemID')['salesPrice'].agg(\n",
    "    mean_price='mean',\n",
    "    median_price='median'\n",
    ").reset_index()\n",
    "\n",
    "# Compute absolute difference\n",
    "price_stats['abs_diff'] = (price_stats['mean_price'] - price_stats['median_price']).abs()\n",
    "\n",
    "# Compute absolute percentage difference relative to median\n",
    "price_stats['abs_perc_diff'] = (price_stats['abs_diff'] / price_stats['median_price']).abs() * 100\n",
    "\n",
    "# Round numerical columns\n",
    "price_stats[['mean_price', 'median_price', 'abs_diff', 'abs_perc_diff']] = price_stats[\n",
    "    ['mean_price', 'median_price', 'abs_diff', 'abs_perc_diff']].round(2)\n",
    "\n",
    "# Sort by absolute percentage difference descending\n",
    "price_stats = price_stats.sort_values(by='abs_perc_diff', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256597e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now look at histograms of top 50 of items with most % difference of mean and median\n",
    "\n",
    "df_sample = price_stats.sort_values(by='abs_perc_diff', ascending=False).head(50)[[\"itemID\"]]\n",
    "\n",
    "df_sample = df_sample.sample(n=50, random_state=222)\n",
    "\n",
    "df_sample = df_sample.sort_values(by=\"itemID\", ascending=True)\n",
    "\n",
    "df_sample = df_sample.merge(df_train, how=\"left\", on=\"itemID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcfd246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing \n",
    "\n",
    "# Unique items\n",
    "item_ids = df_sample['itemID'].unique()\n",
    "\n",
    "# Set up the grid\n",
    "rows, cols = 10, 5\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(20, 15), sharex=False, sharey=False)\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot histogram for each item\n",
    "for i, item_id in enumerate(item_ids):\n",
    "    ax = axes[i]\n",
    "    item_prices = df_sample[df_sample['itemID'] == item_id]['salesPrice']\n",
    "\n",
    "    ax.hist(item_prices, bins=10, color='skyblue', edgecolor='black')\n",
    "    ax.set_title(f'Item {item_id}', fontsize=8)\n",
    "    ax.tick_params(labelsize=6)\n",
    "    ax.grid(True)\n",
    "\n",
    "# Hide unused subplots\n",
    "for j in range(len(item_ids), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e719f780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is better to use mean because the extreme values are not a one-case accidental thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1415bd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate using mean (average), but specifically weighted average, to gain price per each\n",
    "# weighted average will take into account how accidental the outlier prices were"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e49f5b",
   "metadata": {},
   "source": [
    "### Aggregate orders on day level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1227cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to get weighted average price per each for the items\n",
    "# first we need to add a column to df_train\n",
    "# with order value = qty * price\n",
    "\n",
    "df_train[\"orderValue\"] = df_train[\"order\"] * df_train[\"salesPrice\"]\n",
    "\n",
    "# and for test\n",
    "\n",
    "df_test[\"orderValue\"] = df_test[\"order\"] * df_test[\"salesPrice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa9740d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregating on day level\n",
    "\n",
    "df_train_daily = df_train.groupby(['date', 'itemID']).agg(\n",
    "    qty_sold=('order', 'sum'),\n",
    "    sales_value=('orderValue', 'sum'),\n",
    "    promotion=('promotion', 'max'),  # If any transaction had promotion == 1, result will be 1\n",
    "    maxItemPrice=('maxPrice', 'max'), # doesn't matter min or max - its the same value for all transactions with the same item\n",
    "    minItemPrice=('minPrice', 'max') # doesn't matter min or max - its the same value for all transactions with the same item\n",
    ").reset_index()\n",
    "\n",
    "# and for test\n",
    "\n",
    "df_test_daily = df_test.groupby(['date', 'itemID']).agg(\n",
    "    qty_sold=('order', 'sum'),\n",
    "    sales_value=('orderValue', 'sum'),\n",
    "    promotion=('promotion', 'max'),  # If any transaction had promotion == 1, result will be 1\n",
    "    maxItemPrice=('maxPrice', 'max'), # doesn't matter min or max - its the same value for all transactions with the same item\n",
    "    minItemPrice=('minPrice', 'max') # doesn't matter min or max - its the same value for all transactions with the same item\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8ad04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deriving price per each for the items, but specific price for that day\n",
    "\n",
    "df_train_daily[\"DailyItemQty\"] = df_train_daily.groupby([\"itemID\", \"date\"])[\"qty_sold\"].transform(\"sum\")\n",
    "df_train_daily[\"DailyItemValue\"] = df_train_daily.groupby([\"itemID\", \"date\"])[\"sales_value\"].transform(\"sum\")\n",
    "\n",
    "df_train_daily[\"PricePerEachToday\"] = round(df_train_daily[\"DailyItemValue\"] / df_train_daily[\"DailyItemQty\"], 2)\n",
    "\n",
    "df_train_daily = df_train_daily.drop(['DailyItemQty', 'DailyItemValue'], axis=1)\n",
    "\n",
    "# and for test\n",
    "\n",
    "df_test_daily[\"DailyItemQty\"] = df_test_daily.groupby([\"itemID\", \"date\"])[\"qty_sold\"].transform(\"sum\")\n",
    "df_test_daily[\"DailyItemValue\"] = df_test_daily.groupby([\"itemID\", \"date\"])[\"sales_value\"].transform(\"sum\")\n",
    "\n",
    "df_test_daily[\"PricePerEachToday\"] = round(df_test_daily[\"DailyItemValue\"] / df_test_daily[\"DailyItemQty\"], 2)\n",
    "\n",
    "df_test_daily = df_test_daily.drop(['DailyItemQty', 'DailyItemValue'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ab7e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deriving just average price (for the day and as a whole), no QTY used in order to do weighted average\n",
    "\n",
    "df_train_daily[\"Price\"] = round(df_train_daily.groupby(\"itemID\")[\"sales_value\"].transform(\"mean\"), 2)\n",
    "\n",
    "df_train_daily[\"PriceToday\"] = df_train_daily.groupby([\"itemID\", \"date\"])[\"sales_value\"].transform(\"mean\")\n",
    "\n",
    "# and for test\n",
    "\n",
    "df_test_daily[\"Price\"] = round(df_test_daily.groupby(\"itemID\")[\"sales_value\"].transform(\"mean\"), 2)\n",
    "\n",
    "df_test_daily[\"PriceToday\"] = df_test_daily.groupby([\"itemID\", \"date\"])[\"sales_value\"].transform(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63381aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add median discout for the items\n",
    "# we have it currently for each item in df_infos\n",
    "\n",
    "# Select only itemID and medianDiscount from df_infos and merge on itemID\n",
    "df_train_daily = df_train_daily.merge(\n",
    "    df_infos[[\"itemID\", \"medianDiscPerc\"]],\n",
    "    how=\"left\",\n",
    "    on=\"itemID\"\n",
    ")\n",
    "\n",
    "# and for test\n",
    "\n",
    "df_test_daily = df_test_daily.merge(\n",
    "    df_infos[[\"itemID\", \"medianDiscPerc\"]],\n",
    "    how=\"left\",\n",
    "    on=\"itemID\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63d904b",
   "metadata": {},
   "source": [
    "## Add features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c3310a",
   "metadata": {},
   "source": [
    "#### Complete main DF with missing date + item combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3279e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# completing our main data frame with all missing day+item combinations\n",
    "# for them qty_sold = 0, sales_value = 0, promotion = 0, maxItemPrice = maxItemPrice, minItemPrice = minItemPrice\n",
    "# Price = mean Price for that item, medianDiscPerc = medianDiscPerc\n",
    "\n",
    "new_df_train = pd.DataFrame(df_train_daily[\"itemID\"].unique(), columns = [\"itemID\"])\n",
    "\n",
    "# and for test\n",
    "\n",
    "new_df_test = pd.DataFrame(df_test_daily[\"itemID\"].unique(), columns = [\"itemID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639216b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create date range\n",
    "#checkp\n",
    "\n",
    "date_range = pd.date_range(start='2018-01-01', end='2018-06-08', freq='D')\n",
    "\n",
    "# Create DataFrame\n",
    "df_dates_train = pd.DataFrame({'date': date_range})\n",
    "\n",
    "# and for test\n",
    "\n",
    "# Create date range\n",
    "date_range = pd.date_range(start='2018-06-09', end='2018-06-29', freq='D')\n",
    "\n",
    "# Create DataFrame\n",
    "df_dates_test = pd.DataFrame({'date': date_range})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3701829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a dummy key to both DataFrames\n",
    "df_dates_train[\"key\"] = 1\n",
    "new_df_train[\"key\"] = 1\n",
    "\n",
    "# Perform cross join\n",
    "new_df_train = pd.merge(df_dates_train, new_df_train, on=\"key\").drop(\"key\", axis=1)\n",
    "\n",
    "# Sort by date and then itemID\n",
    "new_df_train = new_df_train.sort_values(by=[\"date\", \"itemID\"]).reset_index(drop=True)\n",
    "\n",
    "# and for test\n",
    "df_dates_test[\"key\"] = 1\n",
    "new_df_test[\"key\"] = 1\n",
    "\n",
    "# Perform cross join\n",
    "new_df_test = pd.merge(df_dates_test, new_df_test, on=\"key\").drop(\"key\", axis=1)\n",
    "\n",
    "# Sort by date and then itemID\n",
    "new_df_test = new_df_test.sort_values(by=[\"date\", \"itemID\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c23aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_daily = new_df_train.merge(df_train_daily, how=\"left\", on=[\"date\", \"itemID\"])\n",
    "\n",
    "# and for test\n",
    "\n",
    "df_test_daily = new_df_test.merge(df_test_daily, how=\"left\", on=[\"date\", \"itemID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb56a758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for them qty_sold = 0, sales_value = 0, promotion = 0\n",
    "\n",
    "df_train_daily[[\"qty_sold\", \"sales_value\", \"promotion\"]] = df_train_daily[[\"qty_sold\", \"sales_value\", \"promotion\"]].fillna(0)\n",
    "\n",
    "# and for test\n",
    "\n",
    "df_test_daily[[\"qty_sold\", \"sales_value\", \"promotion\"]] = df_test_daily[[\"qty_sold\", \"sales_value\", \"promotion\"]].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971cb53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maxItemPrice = maxItemPrice, minItemPrice = minItemPrice, Price = mean Price for that item, medianDiscPerc = medianDiscPerc\n",
    "\n",
    "# Fill missing maxItemPrice with the max per itemID\n",
    "df_train_daily[\"maxItemPrice\"] = df_train_daily.groupby(\"itemID\")[\"maxItemPrice\"].transform(lambda x: x.fillna(x.max()))\n",
    "\n",
    "# Fill missing minItemPrice with the max per itemID\n",
    "df_train_daily[\"minItemPrice\"] = df_train_daily.groupby(\"itemID\")[\"minItemPrice\"].transform(lambda x: x.fillna(x.max()))\n",
    "\n",
    "# Fill missing Price with the max per itemID\n",
    "df_train_daily[\"Price\"] = df_train_daily.groupby(\"itemID\")[\"Price\"].transform(lambda x: x.fillna(x.max()))\n",
    "\n",
    "# Fill missing PricePerEachToday with the mean per itemID\n",
    "df_train_daily[\"PricePerEachToday\"] = df_train_daily.groupby(\"itemID\")[\"PricePerEachToday\"].transform(\n",
    "    lambda x: x.fillna(x[df_train_daily.loc[x.index, \"qty_sold\"] != 0].mean())\n",
    ").round(2)\n",
    "\n",
    "# Fill missing PriceToday with the mean per itemID\n",
    "df_train_daily[\"PriceToday\"] = df_train_daily.groupby(\"itemID\")[\"PriceToday\"].transform(\n",
    "    lambda x: x.fillna(x[df_train_daily.loc[x.index, \"qty_sold\"] != 0].mean())\n",
    ").round(2)\n",
    "\n",
    "# Fill missing medianDiscPerc with the max per itemID\n",
    "df_train_daily[\"medianDiscPerc\"] = df_train_daily.groupby(\"itemID\")[\"medianDiscPerc\"].transform(lambda x: x.fillna(x.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668f7c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7784ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and for test\n",
    "\n",
    "# maxItemPrice = maxItemPrice, minItemPrice = minItemPrice, Price = mean Price for that item, medianDiscPerc = medianDiscPerc\n",
    "\n",
    "# Fill missing maxItemPrice with the max per itemID\n",
    "df_test_daily[\"maxItemPrice\"] = df_test_daily.groupby(\"itemID\")[\"maxItemPrice\"].transform(lambda x: x.fillna(x.max()))\n",
    "\n",
    "# Fill missing minItemPrice with the max per itemID\n",
    "df_test_daily[\"minItemPrice\"] = df_test_daily.groupby(\"itemID\")[\"minItemPrice\"].transform(lambda x: x.fillna(x.max()))\n",
    "\n",
    "# Fill missing Price with the max per itemID\n",
    "df_test_daily[\"Price\"] = df_test_daily.groupby(\"itemID\")[\"Price\"].transform(lambda x: x.fillna(x.max()))\n",
    "\n",
    "# Fill missing PricePerEachToday with the mean per itemID\n",
    "df_test_daily[\"PricePerEachToday\"] = df_test_daily.groupby(\"itemID\")[\"PricePerEachToday\"].transform(\n",
    "    lambda x: x.fillna(x[df_test_daily.loc[x.index, \"qty_sold\"] != 0].mean())\n",
    ")\n",
    "df_test_daily[\"PricePerEachToday\"] = df_test_daily[\"PricePerEachToday\"].round(2)\n",
    "\n",
    "# Fill missing PriceToday with the mean per itemID\n",
    "df_test_daily[\"PriceToday\"] = df_test_daily.groupby(\"itemID\")[\"PriceToday\"].transform(\n",
    "    lambda x: x.fillna(x[df_test_daily.loc[x.index, \"qty_sold\"] != 0].mean())\n",
    ")\n",
    "df_test_daily[\"PriceToday\"] = df_test_daily[\"PriceToday\"].round(2)\n",
    "\n",
    "# Fill missing medianDiscPerc with the max per itemID\n",
    "df_test_daily[\"medianDiscPerc\"] = df_test_daily.groupby(\"itemID\")[\"medianDiscPerc\"].transform(lambda x: x.fillna(x.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d895f1b",
   "metadata": {},
   "source": [
    "#### Add masterdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b90805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# include masterdata\n",
    "\n",
    "df_train_daily = df_train_daily.merge(df_items, how=\"left\", on=\"itemID\")\n",
    "\n",
    "# and in test\n",
    "\n",
    "df_test_daily = df_test_daily.merge(df_items, how=\"left\", on=\"itemID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d660dc7a",
   "metadata": {},
   "source": [
    "#### Add date features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca058ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add date features\n",
    "df_train_daily[\"weekDay\"] = df_train_daily[\"date\"].dt.weekday + 1\n",
    "df_train_daily[\"day\"] = df_train_daily[\"date\"].dt.day\n",
    "\n",
    "# and in test\n",
    "\n",
    "df_test_daily[\"weekDay\"] = df_test_daily[\"date\"].dt.weekday + 1\n",
    "df_test_daily[\"day\"] = df_test_daily[\"date\"].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b54baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_week_of_month(date):\n",
    "    # First day of the month\n",
    "    first_day = date.replace(day=1)\n",
    "    \n",
    "    # Find the day of the week the first day lands on (Monday=0, Sunday=6)\n",
    "    first_day_weekday = first_day.weekday()\n",
    "    \n",
    "    # Calendar row index = (day of month + offset from Monday) // 7 + 1\n",
    "    return ((date.day + first_day_weekday - 1) // 7) + 1\n",
    "\n",
    "# Apply the function to create the column\n",
    "df_train_daily['weekOfMonth'] = df_train_daily['date'].apply(get_week_of_month)\n",
    "\n",
    "# and in test\n",
    "df_test_daily['weekOfMonth'] = df_test_daily['date'].apply(get_week_of_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37d5416",
   "metadata": {},
   "source": [
    "### FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4445ffd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_harmonics(data, num_harmonics=10, return_wave=None):\n",
    "    all_coefs = np.fft.fft(data)\n",
    "    coeffs = []\n",
    "    nh = return_wave + 1 if return_wave is not None else num_harmonics\n",
    "    for i in range(1, nh + 1):\n",
    "        coeffs.append(np.zeros(len(all_coefs), dtype=complex))\n",
    "        coeffs[-1][i] = all_coefs[i]\n",
    "        coeffs[-1][-i] = all_coefs[-i]\n",
    "\n",
    "    if return_wave is not None:\n",
    "        rc = np.zeros(len(all_coefs), dtype=complex) + coeffs[return_wave]\n",
    "        rc = np.fft.ifft(rc).real\n",
    "        return rc\n",
    "\n",
    "    reconstructed_coeffs = np.zeros(len(all_coefs), dtype=complex)\n",
    "    for i in range(num_harmonics):\n",
    "        reconstructed_coeffs += coeffs[i]\n",
    "    reconstructed_signal = np.fft.ifft(reconstructed_coeffs).real\n",
    "    reconstructed_signal += data.mean()\n",
    "    return reconstructed_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d40d71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    df_train_daily[f'harmonic_{i}'] = df_train_daily.groupby(by=\"itemID\")[\"qty_sold\"].transform(lambda c: get_harmonics(c, return_wave=i))\n",
    "\n",
    "# and in test\n",
    "\n",
    "for i in range(5):\n",
    "    df_test_daily[f'harmonic_{i}'] = df_test_daily.groupby(by=\"itemID\")[\"qty_sold\"].transform(lambda c: get_harmonics(c, return_wave=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f01728c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_daily.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a20a30",
   "metadata": {},
   "source": [
    "### Cumulative variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ec1d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_daily['cum_sum_order'] = df_train_daily.groupby('itemID')['qty_sold'].cumsum()\n",
    "\n",
    "# and in test\n",
    "\n",
    "df_test_daily['cum_sum_order'] = df_test_daily.groupby('itemID')['qty_sold'].cumsum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f801a5f4",
   "metadata": {},
   "source": [
    "### Rolling statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e88f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# once again, making sure data frame is sorted by date and item ID so the rolling stats are OK\n",
    "\n",
    "df_train_daily.sort_values(['itemID', 'date'], inplace=True)\n",
    "\n",
    "# and in test\n",
    "\n",
    "df_test_daily.sort_values(['itemID', 'date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffdedab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_daily['rolling_qty_sold_mean'] = (\n",
    "    df_train_daily\n",
    "    .groupby('itemID')['qty_sold']\n",
    "    .transform(lambda x: x.shift(1).rolling(window=7).mean())\n",
    ")\n",
    "\n",
    "df_train_daily['rolling_qty_sold_std'] = (\n",
    "    df_train_daily\n",
    "    .groupby('itemID')['qty_sold']\n",
    "    .transform(lambda x: x.shift(1).rolling(window=7).std())\n",
    ")\n",
    "\n",
    "df_train_daily['rolling_qty_sold_median'] = (df_train_daily\n",
    "    .groupby('itemID')['qty_sold']\n",
    "    .transform(lambda x: x.shift(1).rolling(window=7).median())\n",
    ")\n",
    "\n",
    "#def median_of_uniques(x):\n",
    "#    return np.median(np.unique(x))\n",
    "\n",
    "#df_train_daily = df_train_daily.sort_values(['itemID', 'date'])\n",
    "\n",
    "#df_train_daily['rolling_qty_sold_median_distincts'] = (\n",
    "#    df_train_daily\n",
    "#    .groupby('itemID')['qty_sold']\n",
    "#    .transform(lambda x: x.shift(1).rolling(window=7).apply(median_of_uniques, raw=True))\n",
    "#)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d19b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and in test\n",
    "\n",
    "df_test_daily['rolling_qty_sold_mean'] = (\n",
    "    df_test_daily\n",
    "    .groupby('itemID')['qty_sold']\n",
    "    .transform(lambda x: x.shift(1).rolling(window=7).mean())\n",
    ")\n",
    "\n",
    "df_test_daily['rolling_qty_sold_std'] = (\n",
    "    df_test_daily\n",
    "    .groupby('itemID')['qty_sold']\n",
    "    .transform(lambda x: x.shift(1).rolling(window=7).std())\n",
    ")\n",
    "\n",
    "df_test_daily['rolling_qty_sold_median'] = (df_test_daily\n",
    "    .groupby('itemID')['qty_sold']\n",
    "    .transform(lambda x: x.shift(1).rolling(window=7).median())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a8e72c",
   "metadata": {},
   "source": [
    "### Add lagged variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ed6b9a",
   "metadata": {},
   "source": [
    "#### Lagged qty_sold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78d1b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by item and date before shift\n",
    "df_train_daily = df_train_daily.sort_values(by=[\"itemID\", \"date\"])\n",
    "\n",
    "# and in test\n",
    "\n",
    "df_test_daily = df_test_daily.sort_values(by=[\"itemID\", \"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e717cade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at average daily orders and median daily orders of the items\n",
    "# to help decide how many lags are appropriate\n",
    "\n",
    "# define a function to compute the median of unique values\n",
    "def median_of_unique(x):\n",
    "    return np.median(np.unique(x))\n",
    "\n",
    "# compute average daily orders and median of unique daily orders\n",
    "item_daily_stats = (\n",
    "    df_train_daily.groupby('itemID')[\"qty_sold\"]\n",
    "    .agg(\n",
    "        avg_daily_orders='mean',\n",
    "        median_daily_orders_unique=lambda x: median_of_unique(x)\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "item_daily_stats[\"avg_daily_orders\"] = item_daily_stats[\"avg_daily_orders\"].round(2)\n",
    "item_daily_stats[\"median_daily_orders_unique\"] = item_daily_stats[\"median_daily_orders_unique\"].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6754ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I tested a couple of options\n",
    "# settled on 1 lag only to be able to drop the rows with missing values  resutlting from the lag\n",
    "# without losing much data\n",
    "# anyways, we would expect that the sale from the day directly before will be most significat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61088e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lagged variables for qty_sold (currently, just 1)\n",
    "\n",
    "lags = [1, 2, 3, 7]\n",
    "for lag in lags:\n",
    "    df_train_daily[f\"qty_sold_lag{lag}\"] = (\n",
    "        df_train_daily.groupby(\"itemID\")[\"qty_sold\"].shift(lag)\n",
    "    )\n",
    "\n",
    "# and in test\n",
    "\n",
    "lags = [1, 2, 3, 7]\n",
    "for lag in lags:\n",
    "    df_test_daily[f\"qty_sold_lag{lag}\"] = (\n",
    "        df_test_daily.groupby(\"itemID\")[\"qty_sold\"].shift(lag)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d3c0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder lag columns right after qty_sold\n",
    "# Get all column names\n",
    "cols = list(df_train_daily.columns)\n",
    "\n",
    "# Remove lag columns from current position\n",
    "lag_cols = [f\"qty_sold_lag{lag}\" for lag in lags]\n",
    "for col in lag_cols:\n",
    "    cols.remove(col)\n",
    "\n",
    "# Find index of qty_sold\n",
    "qty_idx = cols.index(\"qty_sold\")\n",
    "\n",
    "# Insert lag columns in order after qty_sold\n",
    "for i, col in enumerate(lag_cols):\n",
    "    cols.insert(qty_idx + 1 + i, col)\n",
    "\n",
    "# Reorder DataFrame\n",
    "df_train_daily = df_train_daily[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ff3dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and in test\n",
    "\n",
    "# Reorder lag columns right after qty_sold\n",
    "# Get all column names\n",
    "cols = list(df_test_daily.columns)\n",
    "\n",
    "# Remove lag columns from current position\n",
    "lag_cols = [f\"qty_sold_lag{lag}\" for lag in lags]\n",
    "for col in lag_cols:\n",
    "    cols.remove(col)\n",
    "\n",
    "# Find index of qty_sold\n",
    "qty_idx = cols.index(\"qty_sold\")\n",
    "\n",
    "# Insert lag columns in order after qty_sold\n",
    "for i, col in enumerate(lag_cols):\n",
    "    cols.insert(qty_idx + 1 + i, col)\n",
    "\n",
    "# Reorder DataFrame\n",
    "df_test_daily = df_test_daily[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db30e36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many values will remain if rows containing NA are dropped\n",
    "\n",
    "len(df_train_daily.dropna(subset=[\"qty_sold_lag1\", \"qty_sold_lag2\", \"qty_sold_lag3\", \"qty_sold_lag7\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540e9452",
   "metadata": {},
   "source": [
    "#### Lagged PricePerEachToday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4275061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by item and date before shift\n",
    "df_train_daily = df_train_daily.sort_values(by=[\"itemID\", \"date\"])\n",
    "\n",
    "# and in test\n",
    "\n",
    "df_test_daily = df_test_daily.sort_values(by=[\"itemID\", \"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c74903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lagged variables for PricePerEachToday\n",
    "\n",
    "lags = [1, 2, 3, 7]\n",
    "for lag in lags:\n",
    "    df_train_daily[f\"PricePerEach_lag{lag}\"] = (\n",
    "        df_train_daily.groupby(\"itemID\")[\"PricePerEachToday\"].shift(lag)\n",
    "    )\n",
    "\n",
    "# and in test\n",
    "\n",
    "lags = [1, 2, 3, 7]\n",
    "for lag in lags:\n",
    "    df_test_daily[f\"PricePerEach_lag{lag}\"] = (\n",
    "        df_test_daily.groupby(\"itemID\")[\"PricePerEachToday\"].shift(lag)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0f28f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unlagged column PricePerEachToday\n",
    "\n",
    "df_train_daily = df_train_daily.drop(columns=[\"PricePerEachToday\"])\n",
    "\n",
    "# and in test\n",
    "\n",
    "df_test_daily = df_test_daily.drop(columns=[\"PricePerEachToday\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2083185",
   "metadata": {},
   "source": [
    "#### Lagged sales_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a823e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by item and date before shift\n",
    "df_train_daily = df_train_daily.sort_values(by=[\"itemID\", \"date\"])\n",
    "\n",
    "# and in test\n",
    "\n",
    "df_test_daily = df_test_daily.sort_values(by=[\"itemID\", \"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e37147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lagged variables for PricePerEachToday\n",
    "\n",
    "lags = [1, 2, 3, 7]\n",
    "for lag in lags:\n",
    "    df_train_daily[f\"sales_value_lag{lag}\"] = (\n",
    "        df_train_daily.groupby(\"itemID\")[\"sales_value\"].shift(lag)\n",
    "    )\n",
    "\n",
    "# and in test\n",
    "\n",
    "lags = [1, 2, 3, 7]\n",
    "for lag in lags:\n",
    "    df_test_daily[f\"sales_value_lag{lag}\"] = (\n",
    "        df_test_daily.groupby(\"itemID\")[\"sales_value\"].shift(lag)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf26eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unlagged column PricePerEachToday\n",
    "\n",
    "df_train_daily = df_train_daily.drop(columns=[\"sales_value\"])\n",
    "\n",
    "# and in test\n",
    "\n",
    "df_test_daily = df_test_daily.drop(columns=[\"sales_value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a09901f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many values will remain if rows containing NA are dropped\n",
    "\n",
    "len(df_train_daily.dropna(subset=[\"qty_sold_lag1\", \"qty_sold_lag2\", \"qty_sold_lag3\", \"qty_sold_lag7\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cf68ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what % will remain if rows containing NA are dropped\n",
    "\n",
    "round((len(df_train_daily.dropna(subset=[\"qty_sold_lag1\", \"qty_sold_lag2\", \"qty_sold_lag3\", \"qty_sold_lag7\"]))/len(df_train_daily))*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75faa82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping rows containing NA\n",
    "\n",
    "df_train_daily.dropna(subset=[\"qty_sold_lag1\", \"qty_sold_lag2\", \"qty_sold_lag3\", \"qty_sold_lag7\"], inplace=True)\n",
    "\n",
    "# and in test\n",
    "\n",
    "df_test_daily.dropna(subset=[\"qty_sold_lag1\", \"qty_sold_lag2\", \"qty_sold_lag3\", \"qty_sold_lag7\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e989546f",
   "metadata": {},
   "source": [
    "### Add 2 noise columns for significance testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9140fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# think about whether we want to put some borders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473436ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(22)  # for reproducibility\n",
    "\n",
    "df_train_daily[\"random_noise1\"] = np.random.normal(0, 1, len(df_train_daily))\n",
    "\n",
    "# and in test\n",
    "\n",
    "np.random.seed(23)  # for reproducibility\n",
    "\n",
    "df_test_daily[\"random_noise1\"] = np.random.normal(0, 1, len(df_test_daily))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db45037",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(66)  # for reproducibility\n",
    "\n",
    "df_train_daily[\"random_noise2\"] = np.random.normal(0, 1, len(df_train_daily))\n",
    "\n",
    "# and in test\n",
    "\n",
    "np.random.seed(67)  # for reproducibility\n",
    "\n",
    "df_test_daily[\"random_noise2\"] = np.random.normal(0, 1, len(df_test_daily))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7b577f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some more rounding\n",
    "\n",
    "df_train_daily[\"random_noise1\"] = df_train_daily[\"random_noise1\"].round(2)\n",
    "df_train_daily[\"random_noise2\"] = df_train_daily[\"random_noise2\"].round(2)\n",
    "\n",
    "# and in test\n",
    "\n",
    "df_test_daily[\"random_noise1\"] = df_test_daily[\"random_noise1\"].round(2)\n",
    "df_test_daily[\"random_noise2\"] = df_test_daily[\"random_noise2\"].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91990bc",
   "metadata": {},
   "source": [
    "## Exporting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887ce679",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_daily.to_csv(\"train.csv\", index=False)\n",
    "df_test_daily.to_csv(\"test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c91fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_daily.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af21eaa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
